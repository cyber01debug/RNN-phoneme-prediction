{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение рекуррентной нейронной сети для предсказания следующей фонемы\n",
    "## В данном коде мы будем обучать базовую рекуррентную нейронную сеть с прямым и обратным распространением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import sys\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['j', 'u', \"r'\", 'i'], ['t', \"r'\", 'i', 'f', 'a', 'n', 'a', 'f'], ['a', \"b'\", \"m'\", 'e', 'n'], ['y', 'j', 'u', \"l'\", 'e'], ['m', 'a', 'c']]\n",
      "75235\n"
     ]
    }
   ],
   "source": [
    "# чтение датасета, состоящего из фонетически транскрибированных слов (каждая фонема через пробел)\n",
    "f = open('phon.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# создание пустого списка для токенов\n",
    "tokens = list()\n",
    "for line in raw[0:]:  \n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[0:]) #добавление слов в список токенов\n",
    "\n",
    "# таким образом токены состоят из списков, содержащих слово в транскрипции \n",
    "print(tokens[0:5]) #вывод первых пяти элементов (токенов)\n",
    "print(len(tokens)) #длина списка токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем список существующих элементов из токенов и присваиваем им уникальный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"n'\", 'zh', \"j'\", 'm', \"v'\", \"f'\", 'd', \"t'\", 't', 'j', \"l'\", 'v', \"c'\", 'e', \"sh'\", \"zh'\", 'r', 'sch', 'k', \"p'\", 'y', \"r'\", 'b', 'g', 's', 'i', 'c', 'l', 'p', \"g'\", 'sh', 'ch_', \"s'\", 'o', \"d'\", 'dzh', 'sc', 'n', \"k'\", 'ch', \"m'\", 'h', 'u', 'f', 'a', \"dz'\", \"b'\", \"z'\", \"h'\", 'z']\n",
      "50\n",
      "{\"n'\": 0, 'zh': 1, \"j'\": 2, 'm': 3, \"v'\": 4, \"f'\": 5, 'd': 6, \"t'\": 7, 't': 8, 'j': 9, \"l'\": 10, 'v': 11, \"c'\": 12, 'e': 13, \"sh'\": 14, \"zh'\": 15, 'r': 16, 'sch': 17, 'k': 18, \"p'\": 19, 'y': 20, \"r'\": 21, 'b': 22, 'g': 23, 's': 24, 'i': 25, 'c': 26, 'l': 27, 'p': 28, \"g'\": 29, 'sh': 30, 'ch_': 31, \"s'\": 32, 'o': 33, \"d'\": 34, 'dzh': 35, 'sc': 36, 'n': 37, \"k'\": 38, 'ch': 39, \"m'\": 40, 'h': 41, 'u': 42, 'f': 43, 'a': 44, \"dz'\": 45, \"b'\": 46, \"z'\": 47, \"h'\": 48, 'z': 49}\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "vocab = set() #создается простой список фонем из словаря\n",
    "for sound in tokens: #итерация по каждому элементу в списке 'tokens'\n",
    "    for phon in sound: #теперь по каждому звуку\n",
    "        if phon == '': #проверям не пуста ли фонема, если да пропускаем\n",
    "            pass\n",
    "        else:\n",
    "            vocab.add(phon) #добавляем фонему в 'vocab', если фонема уже есть, добавление не происходит\n",
    "vocab = list(vocab) #преобразование множества в список\n",
    "phon2index = {} #создание пустого словаря для хранения фонемы и ее индекса\n",
    "for i,phon in enumerate(vocab):  #итерация по списку 'vocab', получая индекс i и фонему 'phon' \n",
    "    phon2index[phon]=i\n",
    "    \n",
    "def phons2indices(word): \n",
    "    '''Функция для преобразования списка Фонем в список индексов'''\n",
    "    idx = list() #список для индексов\n",
    "    for phon in word: #проходим по фонемам в слове\n",
    "        if phon == '': #проверяем на пустую строчку\n",
    "            pass\n",
    "        else:\n",
    "            idx.append(phon2index[phon]) #ищем индекс фонемы в словаре 'phon2index' и добавляет его в список 'idx'\n",
    "    return idx\n",
    "\n",
    "def softmax(x): \n",
    "    '''Функция активации softmax для предсказания следующей фонемы'''\n",
    "    e_x = np.exp(x - np.max(x)) #ищем экспоненту каждого элемента вектора, вычитая максимальное значение \n",
    "    return e_x / e_x.sum(axis=0) #нормализуем экспоненциальные значения, деля на их сумму\n",
    "    # возвращает вектор вероятностей\n",
    "\n",
    "# выводим наш список vocab со всеми упомянутыми фонемами\n",
    "print(vocab)\n",
    "# длина списка vocab\n",
    "print(len(vocab))\n",
    "# выводим словарь, где каждой фонеме присвоен уникальный индекс (начиная с 0)\n",
    "print(phon2index)\n",
    "# длина словаря\n",
    "print(len(phon2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделение данных на обучающую и тестовую выборки 80/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(tokens) * 0.9)\n",
    "train_tokens = tokens[:split_index]\n",
    "test_tokens = tokens[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задаем параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32 #размерность векторного представления фонем, то есть фонем из 50 элементов\n",
    "learning_rate = 0.000001 #устанавливаем скорость обучения\n",
    "BATCH_SIZE = 3  #размер батча \n",
    "EPOCHS = 50 #количество эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция для создания батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(tokens, batch_size):\n",
    "    '''Функция принимает список токенов (слов) и размер батча'''\n",
    "    batches = [] #пустой список для батчей\n",
    "    for i in range(0, len(tokens), batch_size): #итерирация по списку токенов с шагом, равным размеру батча\n",
    "        batch = tokens[i:i + batch_size] #извлекаем из списка токена батч\n",
    "        # фильтруем пустые слова и преобразуем в индексы\n",
    "        batch = [phons2indices(word) for word in batch if word]\n",
    "        # фильтруем пустые списки (слова)\n",
    "        batch = [word for word in batch if word]\n",
    "        if batch:  #проверяем, что после фильтрации батч не пуст\n",
    "            batches.append(batch) #добавляем батч\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрицы для предсказания\n",
    "# Инициализируем веса с Xavier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор эмбейдинга: [[-0.06029148 -0.02949953  0.15491302 ...  0.05767308  0.10698856\n",
      "   0.25056396]\n",
      " [-0.06177425  0.22244185 -0.06526564 ...  0.02458765 -0.1638335\n",
      "  -0.14441902]\n",
      " [ 0.25421604 -0.08579621 -0.18921236 ...  0.13001235  0.0007223\n",
      "  -0.15023584]\n",
      " ...\n",
      " [-0.0426686  -0.16901406  0.0611478  ... -0.03253502  0.14617692\n",
      "   0.06697468]\n",
      " [ 0.06970539  0.03440378  0.10694344 ...  0.1777015  -0.10981352\n",
      "   0.09522072]\n",
      " [ 0.19355261  0.09409963 -0.16318258 ... -0.02722063 -0.01306414\n",
      "  -0.3860442 ]]\n",
      "Матрица декодера: [[-0.04702331  0.16689541 -0.14367646 ... -0.17615205  0.10633911\n",
      "   0.15731902]\n",
      " [ 0.15680439 -0.04036213  0.12588732 ... -0.04922388 -0.06079837\n",
      "   0.07691231]\n",
      " [-0.19707566  0.10661243  0.15829207 ... -0.20044005 -0.03706107\n",
      "  -0.2334421 ]\n",
      " ...\n",
      " [-0.17237103  0.17246761  0.18118204 ... -0.11623114 -0.22374876\n",
      "   0.3944393 ]\n",
      " [-0.01272492  0.02847676  0.300425   ... -0.16973205  0.0693595\n",
      "   0.1970073 ]\n",
      " [-0.2703098   0.02379944  0.14974037 ...  0.14218018  0.06873423\n",
      "   0.20702878]]\n"
     ]
    }
   ],
   "source": [
    "'''Создаем матрицу эмбеддингов (векторных представлений) для фонем. \n",
    "Инициализируем матрицу случайными значениями из нормального распределения со средним значением 0 и стандартным отклонением, \n",
    "рассчитанным на основе размера словаря фонем размер матрицы = (количество фонем в словаре, размер эмбеддинга)'''\n",
    "embed = np.random.normal(0, 1/np.sqrt(len(vocab)), size=(len(vocab), embed_size))\n",
    "\n",
    "'''Создаем рекуррентную матрицу, инициализируем матрицу случайными значениями из нормального распределения со средним значением 0\n",
    "и стандартным отклонением, рассчитанным на основе размера эмбеддинга размер матрицы = (размер эмбеддинга, размер эмбеддинга)'''\n",
    "recurrent = np.random.normal(0, 1/np.sqrt(embed_size), size=(embed_size, embed_size))\n",
    "\n",
    "'''Создаем начальный вектор скрытого состояния и инициализируем вектор нулями размер вектора = (размер эмбеддинга)'''\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "'''Создаем матрицу декодера. Инициализируем матрицу случайными значениями из нормального распределения со средним значением 0\n",
    "и стандартным отклонением, рассчитанным на основе размера эмбеддинга размер матрицы = (размер эмбеддинга, количество фонем в словаре)'''\n",
    "decoder = np.random.normal(0, 1/np.sqrt(embed_size), size=(embed_size, len(vocab)))\n",
    "\n",
    "'''Создаем матрицу one-hot encoding. Создаем единичную матрицу, которая используется для представления фонем в виде one-hot вектора\n",
    "размер матрицы = (количество фонем в словаре, количество фонем в словаре)'''\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "\n",
    "print('Вектор эмбейдинга:', embed)\n",
    "print('Матрица декодера:', decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    '''Функция принимает на вход список индексов фонем (слова)'''    \n",
    "    layers = list() #Список с именем layers для прямого распространения\n",
    "    layer = {} #словарб для слоёв\n",
    "    layer['hidden'] = start #скрытое состояние первого слоя начальным вектором 'start'\n",
    "    layers.append(layer) #добавляем первый слой в список\n",
    "    loss = 0 #для хранения суммарной ошибки\n",
    "    correct_predictions = 0 #правильные предсказания \n",
    "    total_predictions = 0 #все предсказания\n",
    "\n",
    "    # итерация по слову\n",
    "    for target_i in range(len(word)): #проходим по всем фонемам в слове\n",
    "        layer = {} #словарь для этого слоя\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) #извлекает вероятность, которую сеть предсказала для следующей фонемы  \n",
    "\n",
    "        # вычисляем ошибку для текущего предсказания, используя отрицательный логарифм вероятности правильного слова, добавляем ошибку к общей суммарной ошибке 'loss'\n",
    "        # epsilon = 1e-15 #можно добавить маленькое число, которое используется для предотвращения взятия логарифма от нуля\n",
    "        loss += -np.log(layer['pred'][word[target_i]]) #чем ниже вероятность предсказанного слова, тем выше значение loss\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[word[target_i]] \n",
    "        layers.append(layer) \n",
    "\n",
    "        predicted_index = np.argmax(layer['pred']) #предсказываем индекс следующей фонемы \n",
    "        if predicted_index == word[target_i]: #если предсказанная фонема совпадает с правильной, то увеличиваем количество верных предсказаний\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1 #увеличиваем общее количество предсказаний\n",
    "\n",
    "    # вычисляем точность модели \n",
    "    # делим количество верных предсказаний на общее количество предсказаний\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    # возвращаем результат (слои потерю и точность)\n",
    "    return layers, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем папку для сохранения весов молели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь для сохранения весов модели\n",
    "save_path = \"weights\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path) #если папки нет создаем \n",
    "# Переменные для хранения лучших результатов модели\n",
    "best_accuracy = 0.0 #\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1/50, Обучение: Loss: 21.27, Accuracy: 0.03 , Тест: Loss: 21.26, Accuracy: 0.03\n",
      "Веса обновлены\n",
      "Эпоха 2/50, Обучение: Loss: 21.27, Accuracy: 0.03 , Тест: Loss: 21.26, Accuracy: 0.03\n",
      "Веса обновлены\n",
      "Эпоха 3/50, Обучение: Loss: 21.26, Accuracy: 0.03 , Тест: Loss: 21.25, Accuracy: 0.03\n",
      "Веса обновлены\n",
      "Эпоха 4/50, Обучение: Loss: 21.26, Accuracy: 0.03 , Тест: Loss: 21.25, Accuracy: 0.03\n",
      "Эпоха 5/50, Обучение: Loss: 21.25, Accuracy: 0.03 , Тест: Loss: 21.24, Accuracy: 0.03\n",
      "Эпоха 6/50, Обучение: Loss: 21.25, Accuracy: 0.03 , Тест: Loss: 21.24, Accuracy: 0.03\n",
      "Эпоха 7/50, Обучение: Loss: 21.24, Accuracy: 0.03 , Тест: Loss: 21.23, Accuracy: 0.03\n",
      "Эпоха 8/50, Обучение: Loss: 21.24, Accuracy: 0.03 , Тест: Loss: 21.23, Accuracy: 0.03\n",
      "Эпоха 9/50, Обучение: Loss: 21.23, Accuracy: 0.03 , Тест: Loss: 21.22, Accuracy: 0.03\n",
      "Эпоха 10/50, Обучение: Loss: 21.23, Accuracy: 0.03 , Тест: Loss: 21.22, Accuracy: 0.03\n",
      "Эпоха 11/50, Обучение: Loss: 21.22, Accuracy: 0.03 , Тест: Loss: 21.22, Accuracy: 0.03\n",
      "Эпоха 12/50, Обучение: Loss: 21.22, Accuracy: 0.03 , Тест: Loss: 21.21, Accuracy: 0.03\n",
      "Эпоха 13/50, Обучение: Loss: 21.21, Accuracy: 0.03 , Тест: Loss: 21.21, Accuracy: 0.03\n",
      "Эпоха 14/50, Обучение: Loss: 21.21, Accuracy: 0.03 , Тест: Loss: 21.20, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 15/50, Обучение: Loss: 21.20, Accuracy: 0.04 , Тест: Loss: 21.20, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 16/50, Обучение: Loss: 21.20, Accuracy: 0.04 , Тест: Loss: 21.19, Accuracy: 0.04\n",
      "Эпоха 17/50, Обучение: Loss: 21.19, Accuracy: 0.04 , Тест: Loss: 21.19, Accuracy: 0.04\n",
      "Эпоха 18/50, Обучение: Loss: 21.19, Accuracy: 0.04 , Тест: Loss: 21.18, Accuracy: 0.04\n",
      "Эпоха 19/50, Обучение: Loss: 21.19, Accuracy: 0.04 , Тест: Loss: 21.18, Accuracy: 0.04\n",
      "Эпоха 20/50, Обучение: Loss: 21.18, Accuracy: 0.04 , Тест: Loss: 21.17, Accuracy: 0.04\n",
      "Эпоха 21/50, Обучение: Loss: 21.18, Accuracy: 0.04 , Тест: Loss: 21.17, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 22/50, Обучение: Loss: 21.17, Accuracy: 0.04 , Тест: Loss: 21.17, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 23/50, Обучение: Loss: 21.17, Accuracy: 0.04 , Тест: Loss: 21.16, Accuracy: 0.04\n",
      "Эпоха 24/50, Обучение: Loss: 21.16, Accuracy: 0.04 , Тест: Loss: 21.16, Accuracy: 0.04\n",
      "Эпоха 25/50, Обучение: Loss: 21.16, Accuracy: 0.04 , Тест: Loss: 21.15, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 26/50, Обучение: Loss: 21.15, Accuracy: 0.04 , Тест: Loss: 21.14, Accuracy: 0.04\n",
      "Веса обновлены\n",
      "Эпоха 27/50, Обучение: Loss: 21.14, Accuracy: 0.05 , Тест: Loss: 21.14, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 28/50, Обучение: Loss: 21.14, Accuracy: 0.05 , Тест: Loss: 21.13, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 29/50, Обучение: Loss: 21.13, Accuracy: 0.05 , Тест: Loss: 21.13, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 30/50, Обучение: Loss: 21.13, Accuracy: 0.05 , Тест: Loss: 21.12, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 31/50, Обучение: Loss: 21.12, Accuracy: 0.05 , Тест: Loss: 21.12, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 32/50, Обучение: Loss: 21.12, Accuracy: 0.05 , Тест: Loss: 21.11, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 33/50, Обучение: Loss: 21.11, Accuracy: 0.05 , Тест: Loss: 21.10, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 34/50, Обучение: Loss: 21.10, Accuracy: 0.05 , Тест: Loss: 21.10, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 35/50, Обучение: Loss: 21.09, Accuracy: 0.05 , Тест: Loss: 21.09, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 36/50, Обучение: Loss: 21.09, Accuracy: 0.05 , Тест: Loss: 21.08, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 37/50, Обучение: Loss: 21.08, Accuracy: 0.05 , Тест: Loss: 21.07, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 38/50, Обучение: Loss: 21.07, Accuracy: 0.05 , Тест: Loss: 21.06, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 39/50, Обучение: Loss: 21.06, Accuracy: 0.05 , Тест: Loss: 21.06, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 40/50, Обучение: Loss: 21.05, Accuracy: 0.05 , Тест: Loss: 21.05, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 41/50, Обучение: Loss: 21.04, Accuracy: 0.05 , Тест: Loss: 21.04, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 42/50, Обучение: Loss: 21.03, Accuracy: 0.05 , Тест: Loss: 21.03, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 43/50, Обучение: Loss: 21.02, Accuracy: 0.05 , Тест: Loss: 21.01, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 44/50, Обучение: Loss: 21.01, Accuracy: 0.05 , Тест: Loss: 21.00, Accuracy: 0.05\n",
      "Веса обновлены\n",
      "Эпоха 45/50, Обучение: Loss: 21.00, Accuracy: 0.06 , Тест: Loss: 20.99, Accuracy: 0.06\n",
      "Веса обновлены\n",
      "Эпоха 46/50, Обучение: Loss: 20.99, Accuracy: 0.06 , Тест: Loss: 20.98, Accuracy: 0.06\n",
      "Веса обновлены\n",
      "Эпоха 47/50, Обучение: Loss: 20.97, Accuracy: 0.07 , Тест: Loss: 20.96, Accuracy: 0.07\n",
      "Веса обновлены\n",
      "Эпоха 48/50, Обучение: Loss: 20.96, Accuracy: 0.07 , Тест: Loss: 20.95, Accuracy: 0.07\n",
      "Веса обновлены\n",
      "Эпоха 49/50, Обучение: Loss: 20.94, Accuracy: 0.07 , Тест: Loss: 20.93, Accuracy: 0.07\n",
      "Веса обновлены\n",
      "Эпоха 50/50, Обучение: Loss: 20.92, Accuracy: 0.07 , Тест: Loss: 20.91, Accuracy: 0.07\n",
      "Веса обновлены\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # создаем батчи из обучающих данных\n",
    "    train_batches = create_batches(train_tokens, BATCH_SIZE)\n",
    "   \n",
    "    # задаем переменню общей ошибки, точности, слов \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_words = 0 \n",
    "\n",
    "    for batch in train_batches: #цикл для бача на обучающей выборке (задаем все по нулям)\n",
    "        batch_loss = 0 \n",
    "        batch_accuracy = 0 \n",
    "        batch_words = 0\n",
    "        \n",
    "        for word in batch: #цикл для каждого слова в батче\n",
    "            '''Получаем результаты предсказания модели:\n",
    "            layers: список слоев, содержащих предсказанные значения и другие данные\n",
    "            loss: значение ошибки для данного слова\n",
    "            accuracy: значение точности для данного слова'''\n",
    "            layers, loss, accuracy = predict(word) \n",
    "            batch_loss += loss\n",
    "            batch_accuracy += accuracy\n",
    "            batch_words += 1\n",
    "                \n",
    "            # обратное распространение для обновления весов (вычисляем градиенты ошибки и используем их для корректировки весов модели)\n",
    "            for layer_idx in reversed(range(len(layers))):\n",
    "                # итерируем по слоям в обратном порядке\n",
    "                layer = layers[layer_idx] \n",
    "                # получаем текущий слой\n",
    "                target = word[layer_idx-1]\n",
    "\n",
    "                # проверяем, что слой не первый (входной)\n",
    "                if(layer_idx > 0): \n",
    "                    layer['output_delta'] = layer['pred'] - one_hot[target] #вычисляем разницу между предсказанным и ожидаемым вектором\n",
    "                    new_hidden_delta = layer['output_delta'].dot(decoder.transpose()) #вычисляем ошибку скрытого состояния текущего слоя (транспонируя матрицу весов decoder и перемножая с дельтой выходного слоя)\n",
    "                    \n",
    "                    # проверяем, что слой не последний (выходной)\n",
    "                    if(layer_idx == len(layers)-1):\n",
    "                        # устанавливаем ошибку скрытого слоя равной вычисленной ошибке\n",
    "                        layer['hidden_delta'] = new_hidden_delta\n",
    "                    else:\n",
    "                        # ошибка скрытого состояния вычисляется на основе ошибки текущего слоя и ошибки скрытого состояния следующего слоя\n",
    "                        layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "                else: #если слой первый \n",
    "                    # вычисляем ошибку скрытого состояния первого слоя на основе ошибки скрытого состояния второго слоя\n",
    "                    layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "\n",
    "            # Ограничение градиента (для предотвращения градиентного взрыва)\n",
    "            for layer in layers: #цикл по слоям\n",
    "                if 'hidden_delta' in layer: #проверяем, есть ли ошибка скрытого слоя в текущем слое\n",
    "                    grad_norm = np.linalg.norm(layer['hidden_delta']) # вычисляем норму градиента\n",
    "                    if grad_norm > 10: #устанавливаем максимальную норму\n",
    "                        layer['hidden_delta'] = layer['hidden_delta'] * (10 / grad_norm) # нормализуем градиент если он большой\n",
    "\n",
    "            '''теперь корректируем веса'''\n",
    "            # обновляем начальное скрытое состояние на основе ошибки скрытого состояния первого слоя, деля на длину предложения для нормализации\n",
    "            # вычитаем из начального скрытого состояния (start) произведение ошибки скрытого состояния первого слоя, скорости обучения и деленное на длину предложения.\n",
    "            start -= layers[0]['hidden_delta'] * learning_rate / (len(word))\n",
    "            # итерация по всем слоям со второго \n",
    "            for layer_idx,layer in enumerate(layers[1:]):\n",
    "                # корректируем матрицу декодера с учетом скрытого состояния текущего слоя и ошибки выходного слоя, деля на длину слова для нормализации\n",
    "                decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * learning_rate / (len(word))\n",
    "                # извлекаем индекс текущего слова из списка индексов предложения\n",
    "                embed_idx = word[layer_idx]\n",
    "                # обновляем вектор эмбеддинга текущего слова (embed[embed_idx]), вычитая из текущего значения вектора произведение ошибки скрытого состояния, скорости обучения и деленное на длину слова\n",
    "                # корректирует вектор эмбеддинга для уменьшения ошибки\n",
    "                embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * learning_rate / (len(word)) #корректируем вектор эмбеддинга текущего слова с помощью ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "                # обновляем матрицу рекуррентной связи, вычитая из текущего значения матрицы внешнее произведение скрытого состояния и ошибки скрытого состояния, умноженное на скорость обучения и деленное на длину предложения\n",
    "                # корректирует матрицу рекуррентной связи для уменьшения ошибки при обновлении скрытого состояния\n",
    "                recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * learning_rate / (len(word)) #обновляем матрицу рекуррентной связи с помощью скрытого состояния текущего слоя и ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "            \n",
    "        if batch_words > 0: #проверка на пустые батчи\n",
    "            total_loss += batch_loss #суммируем ошибки для всего батча\n",
    "            total_accuracy += batch_accuracy #суммируем точность для всего батча\n",
    "            total_words += batch_words #суммируем количество слов в батче\n",
    "    if total_words > 0: #проверяем было ли обработано хоть одно слово в текущей эпохе\n",
    "         avg_loss = total_loss/total_words #среднее значение ошибки\n",
    "         avg_accuracy = total_accuracy/total_words #среднее значение точности\n",
    "    else: #если в эпохе не было обработано слов, то средние значения равны нулю\n",
    "         avg_loss = 0\n",
    "         avg_accuracy = 0\n",
    "\n",
    "    # оценка модели на тестовой выборке\n",
    "    test_accuracy = 0\n",
    "    test_words = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    # создаем батчи для тестовой выборки\n",
    "    test_batches = create_batches(test_tokens, BATCH_SIZE)\n",
    "    # цикл по тестовым батчам \n",
    "    for test_batch in test_batches:\n",
    "        batch_test_accuracy = 0 #переменная для хранения точности текущего тестового батча\n",
    "        batch_test_loss = 0 #переменная для хранения потери текущего тестового батча\n",
    "        batch_test_words = 0 #переменняа для хранения количсетва слов в тестовом батче\n",
    "\n",
    "        # цикл по всем словам в тестовом батче\n",
    "        for word in test_batch:\n",
    "            layers, loss, accuracy = predict(word) #получаем предсказания модели, ошибку и точность для текущего слова\n",
    "            batch_test_accuracy += accuracy #добавляем точность \n",
    "            batch_test_loss += loss #добавляем потерю\n",
    "            batch_test_words += 1 #бобавляем счет слов\n",
    "\n",
    "        # проверка на наличие хотя бы одного слова в батче и так же суммируем\n",
    "        if batch_test_words > 0:\n",
    "            test_accuracy += batch_test_accuracy \n",
    "            test_loss += batch_test_loss\n",
    "            test_words += batch_test_words\n",
    "\n",
    "    # проверяем, было ли обработано хотя бы одно слово в тестовой выборке\n",
    "    if test_words > 0:\n",
    "        avg_test_accuracy = test_accuracy/test_words #вычисляем среднюю точность, деля тестовую точность на количество слов\n",
    "        avg_test_loss = test_loss/test_words #вычисляем среднюю ошибку, деля тестовую ошибку на количество слов\n",
    "    # если не было обработано ни одного слова средние значения 0\n",
    "    else:\n",
    "        avg_test_accuracy = 0\n",
    "        avg_test_loss = 0\n",
    "          \n",
    "    # вывод процесса обучения\n",
    "    print(f\"Эпоха {epoch + 1}/{EPOCHS}, \"\n",
    "          f\"Обучение: Loss: {avg_loss:.2f}, Accuracy: {avg_accuracy:.2f} , \"\n",
    "          f\"Тест: Loss: {avg_test_loss:.2f}, Accuracy: {avg_test_accuracy:.2f}\")\n",
    "    # прерываем обучение если среднняя ошибка nan \n",
    "    if np.isnan(avg_loss): \n",
    "        print('Ошибка: nan')\n",
    "        break  \n",
    "    \n",
    "    # проверка на улучшение\n",
    "    # ниже мы проверяем улучшилась ли модель от эпохи к эпохе \n",
    "    # устанавливая условие - либо текущая точность на тестовой выборке (avg_test_accuracy) больше лучшей предыдущей (best_accuracy), \n",
    "    # либо текущая точность равна лучшей, но текущая ошибка на тестовой выборке (avg_test_loss) меньше лучшей предыдущей (best_loss)\n",
    "    if avg_test_accuracy > best_accuracy or (avg_test_accuracy == best_accuracy and avg_test_loss < best_loss):\n",
    "            # обновляем лучшую точность на ту, которую получили по условию\n",
    "            best_accuracy = avg_test_accuracy\n",
    "            # так же с потерей\n",
    "            best_loss = avg_test_loss\n",
    "            # сохранение весов в нашу папку для весов\n",
    "            np.save(os.path.join(save_path, \"embed.npy\"), embed)\n",
    "            np.save(os.path.join(save_path, \"recurrent.npy\"), recurrent)\n",
    "            np.save(os.path.join(save_path, \"start.npy\"), start)\n",
    "            np.save(os.path.join(save_path, \"decoder.npy\"), decoder)\n",
    "            print('Веса обновлены')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попросим сеть предсказать следующую фонему в слове с индексом n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', \"m'\", 'i', 't', \"r'\", 'e', 'e', 'v', 'a']\n",
      "Предыдущая фонема: d Истинная: m' Предсказанная: l'\n",
      "Предыдущая фонема: m' Истинная: i Предсказанная: m\n",
      "Предыдущая фонема: i Истинная: t Предсказанная: b\n",
      "Предыдущая фонема: t Истинная: r' Предсказанная: g\n",
      "Предыдущая фонема: r' Истинная: e Предсказанная: a\n",
      "Предыдущая фонема: e Истинная: e Предсказанная: t\n",
      "Предыдущая фонема: e Истинная: v Предсказанная: h'\n",
      "Предыдущая фонема: v Истинная: a Предсказанная: u\n"
     ]
    }
   ],
   "source": [
    "word_index = 5\n",
    "# получаем предсказания модели для слова с индексом word_index, преобразуя его фонемы в числовые индексы\n",
    "l, _, _ = predict(phons2indices(tokens[word_index]))\n",
    "print(tokens[word_index])\n",
    "# цикл по всем слоям в списке предсказаний (l), кроме входного и выходного слоя \n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    # получаем входную фонему для текущего слоя из оригинального слова\n",
    "    input_phon = tokens[word_index][i]\n",
    "    # получаем настоящую фонему для текущего слоя из оригинального слова\n",
    "    true_phon = tokens[word_index][i + 1]\n",
    "    # получаем предсказаную фонему для текущего слоя из оригинального слова (argmax() используется для нахождения индекса максимальной вероятности в предсказании)\n",
    "    pred_phon = vocab[each_layer['pred'].argmax()]\n",
    "    print(f\"Предыдущая фонема: {input_phon} Истинная: {true_phon} Предсказанная: {pred_phon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'a', \"l'\", 'i', 'zh', 'e', 'l', 'a']\n",
      "Предыдущая фонема: p Истинная: r Предсказанная: l'\n",
      "Предыдущая фонема: r Истинная: a Предсказанная: a\n",
      "Предыдущая фонема: a Истинная: l' Предсказанная: t\n",
      "Предыдущая фонема: l' Истинная: i Предсказанная: l'\n",
      "Предыдущая фонема: i Истинная: zh Предсказанная: a\n",
      "Предыдущая фонема: zh Истинная: e Предсказанная: n\n",
      "Предыдущая фонема: e Истинная: l Предсказанная: e\n",
      "Предыдущая фонема: l Истинная: a Предсказанная: p\n"
     ]
    }
   ],
   "source": [
    "word_index = 15\n",
    "l, _, _ = predict(phons2indices(tokens[word_index]))\n",
    "print(tokens[word_index])\n",
    "# цикл по всем слоям в списке предсказаний (l), кроме входного и выходного слоя \n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    # получаем входную фонему для текущего слоя из оригинального слова\n",
    "    input_phon = tokens[word_index][i]\n",
    "    # получаем настоящую фонему для текущего слоя из оригинального слова\n",
    "    true_phon = tokens[word_index][i + 1]\n",
    "    # получаем предсказаную фонему для текущего слоя из оригинального слова\n",
    "    pred_phon = vocab[each_layer['pred'].argmax()]\n",
    "    print(f\"Предыдущая фонема: {input_phon} Истинная: {true_phon} Предсказанная: {pred_phon}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
