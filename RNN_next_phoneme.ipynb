{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение рекуррентной нейронной сети для предсказания следующей фонемы\n",
    "## В данном коде мы будем обучать базовую рекуррентную нейронную сеть с прямым и обратным распространением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import sys\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['j', 'u', \"r'\", 'i'], ['t', \"r'\", 'i', 'f', 'a', 'n', 'a', 'f'], ['a', \"b'\", \"m'\", 'e', 'n'], ['y', 'j', 'u', \"l'\", 'e'], ['m', 'a', 'c']]\n",
      "75235\n"
     ]
    }
   ],
   "source": [
    "# чтение датасета, состоящего из фонетически транскрибированных слов (каждая фонема через пробел)\n",
    "f = open('phon.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# создание пустого списка для токенов\n",
    "tokens = list()\n",
    "for line in raw[0:]:  \n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[0:]) #добавление слов в список токенов\n",
    "\n",
    "# таким образом токены состоят из списков, содержащих слово в транскрипции \n",
    "print(tokens[0:5]) #вывод первых пяти элементов (токенов)\n",
    "print(len(tokens)) #длина списка токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем список существующих элементов из токенов и присваиваем им уникальный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', 't', 's', 'l', \"zh'\", \"dz'\", \"sh'\", 'p', 'e', 'n', 'y', \"f'\", 'j', 'b', \"l'\", \"b'\", 'dzh', \"m'\", 'ch_', 'k', 'u', 'i', 'z', 'c', \"r'\", 'm', 'zh', \"v'\", 'sh', \"j'\", \"t'\", 'sch', \"n'\", \"z'\", 'ch', \"c'\", 'a', 'f', 'r', \"s'\", 'sc', 'o', \"k'\", 'h', \"p'\", \"d'\", 'v', \"h'\", \"g'\", 'd']\n",
      "50\n",
      "{'g': 0, 't': 1, 's': 2, 'l': 3, \"zh'\": 4, \"dz'\": 5, \"sh'\": 6, 'p': 7, 'e': 8, 'n': 9, 'y': 10, \"f'\": 11, 'j': 12, 'b': 13, \"l'\": 14, \"b'\": 15, 'dzh': 16, \"m'\": 17, 'ch_': 18, 'k': 19, 'u': 20, 'i': 21, 'z': 22, 'c': 23, \"r'\": 24, 'm': 25, 'zh': 26, \"v'\": 27, 'sh': 28, \"j'\": 29, \"t'\": 30, 'sch': 31, \"n'\": 32, \"z'\": 33, 'ch': 34, \"c'\": 35, 'a': 36, 'f': 37, 'r': 38, \"s'\": 39, 'sc': 40, 'o': 41, \"k'\": 42, 'h': 43, \"p'\": 44, \"d'\": 45, 'v': 46, \"h'\": 47, \"g'\": 48, 'd': 49}\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "vocab = set() #создается простой список фонем из словаря\n",
    "for sound in tokens: #итерация по каждому элементу в списке 'tokens'\n",
    "    for phon in sound: #теперь по каждому звуку\n",
    "        if phon == '': #проверям не пуста ли фонема, если да пропускаем\n",
    "            pass\n",
    "        else:\n",
    "            vocab.add(phon) #добавляем фонему в 'vocab', если фонема уже есть, добавление не происходит\n",
    "vocab = list(vocab) #преобразование множества в список\n",
    "phon2index = {} #создание пустого словаря для хранения фонемы и ее индекса\n",
    "for i,phon in enumerate(vocab):  #итерация по списку 'vocab', получая индекс i и фонему 'phon' \n",
    "    phon2index[phon]=i\n",
    "    \n",
    "def phons2indices(word): \n",
    "    '''Функция для преобразования списка Фонем в список индексов'''\n",
    "    idx = list() #список для индексов\n",
    "    for phon in word: #проходим по фонемам в слове\n",
    "        idx.append(phon2index[phon]) #ищем индекс фонемы в словаре 'phon2index' и добавляет его в список 'idx'\n",
    "    return idx\n",
    "\n",
    "def softmax(x): \n",
    "    '''Функция активации softmax для предсказания следующей фонемы'''\n",
    "    e_x = np.exp(x - np.max(x)) #ищем экспоненту каждого элемента вектора, вычитая максимальное значение \n",
    "    return e_x / e_x.sum(axis=0) #нормализуем экспоненциальные значения, деля на их сумму\n",
    "    # возвращает вектор вероятностей\n",
    "\n",
    "# выводим наш список vocab со всеми упомянутыми фонемами\n",
    "print(vocab)\n",
    "# длина списка vocab\n",
    "print(len(vocab))\n",
    "# выводим словарь, где каждой фонеме присвоен уникальный индекс (начиная с 0)\n",
    "print(phon2index)\n",
    "# длина словаря\n",
    "print(len(phon2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрицы для предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор эмбейдинга:\n",
      " [[0.22599767 0.21025573 0.619817   0.10181126 0.82417756 0.35229479\n",
      "  0.17420087 0.89946999 0.47737445 0.48795129 0.53865006 0.58937165\n",
      "  0.26483124 0.98432685 0.59957953]\n",
      " [0.02251275 0.64788816 0.48913721 0.32254489 0.66841533 0.26877149\n",
      "  0.09538665 0.61338554 0.26914206 0.73395576 0.14828368 0.16010576\n",
      "  1.00151956 0.53834975 0.0566615 ]\n",
      " [0.32805772 0.92629265 0.59167864 0.1310873  0.66598231 1.00058382\n",
      "  0.71412372 0.80119982 0.96412475 0.43115836 0.3856158  0.0533974\n",
      "  0.48260591 0.30240927 0.65657666]\n",
      " [0.58408365 0.33831098 0.92089239 0.7358892  0.55880837 0.12016441\n",
      "  0.384233   0.64509949 0.42492332 0.67311826 0.38499143 0.63442764\n",
      "  0.74707303 0.62388644 0.20646057]\n",
      " [0.63428399 0.70506957 0.07214126 0.97559851 0.56698691 0.24504028\n",
      "  0.90514839 0.66933127 0.54624535 0.9113055  0.88417847 0.88768452\n",
      "  0.07196143 0.18350256 0.69507373]]\n",
      "Матрица декодера:\n",
      " [[0.45525961 0.83517141 0.59554007 0.15524546 0.51143109 0.84368874\n",
      "  0.42204687 0.49355248 0.85897441 0.55141218 0.66348394 0.09719797\n",
      "  1.00216729 0.87391065 0.85287851 0.29607379 0.5466097  0.33060378\n",
      "  0.67425126 0.46820354 0.07628148 0.51963551 0.58899259 0.97522579\n",
      "  1.00376212 0.81648196 0.38256278 0.00524594 0.90891747 0.04895018\n",
      "  0.41130063 0.74263254 0.76898564 0.1652938  0.39341482 0.73135912\n",
      "  0.99772446 0.89933129 0.01080501 0.16760753 0.21083935 0.10906614\n",
      "  0.45519267 0.22928958 0.66778518 0.84508819 0.04448568 0.58862309\n",
      "  0.49664532 0.52183744]\n",
      " [0.57299316 0.84046839 0.76871775 0.31794833 0.75964384 0.28299105\n",
      "  0.96379715 0.12243072 0.94004415 1.0017464  0.06805271 0.74076618\n",
      "  0.47949457 0.72640556 0.14702495 0.49205374 0.85722145 0.24039849\n",
      "  0.99572698 0.98870147 0.35531413 0.17243736 0.69740154 0.6409355\n",
      "  0.43592313 0.35465975 0.12942592 0.8109377  0.05083296 0.30498598\n",
      "  0.53397721 0.28154884 0.67996049 0.11892156 0.85269345 0.82825803\n",
      "  0.24897403 0.2239652  0.32545633 0.62239655 0.08029568 0.17893352\n",
      "  0.71746445 0.53908735 0.71800473 0.3967291  0.34581512 0.67601874\n",
      "  0.10126479 0.97368118]\n",
      " [0.18720047 0.72166043 0.29213634 0.75928324 0.15262221 0.18390096\n",
      "  0.26494007 0.20999369 0.47301174 0.7769881  0.07922135 0.54847265\n",
      "  0.17353089 0.04082872 0.21065674 0.52973244 0.29656042 0.75584992\n",
      "  0.89609268 0.56548799 0.98403094 0.04906618 0.03136031 0.85429389\n",
      "  0.33346509 0.38638841 0.39965509 0.84476367 0.30811188 0.11333086\n",
      "  0.59543357 0.9279673  0.374982   0.47360965 0.94960685 0.43606925\n",
      "  0.64637023 0.16209246 0.25339105 0.12626051 0.65253456 0.71071546\n",
      "  0.38129583 0.52930943 0.37359986 0.86243331 0.27963454 0.32170925\n",
      "  0.04531827 0.50842237]\n",
      " [0.30734535 0.30983887 0.7055195  0.28101976 0.76603287 0.27336515\n",
      "  0.61654733 0.52835548 0.65019617 0.79557284 0.43335718 0.34999565\n",
      "  0.1418319  0.8004252  0.52024251 0.14890782 0.37523879 0.35073845\n",
      "  0.20585594 0.78976412 0.16925432 0.80449762 0.60063347 0.47767627\n",
      "  0.83907898 0.84754762 0.65895812 0.02152369 0.55384085 0.08981401\n",
      "  0.98954485 0.25088099 0.44404325 0.55367425 0.4606166  0.9562633\n",
      "  0.43519937 0.64962437 0.38425377 0.96515732 0.72124142 0.726184\n",
      "  0.94813406 0.66627799 0.75686077 0.8456004  0.03615195 0.77549141\n",
      "  0.73872058 0.90877851]\n",
      " [0.56528959 0.56334778 0.60568001 0.92948181 0.47033943 0.79037422\n",
      "  0.87718649 0.44512689 0.48392556 0.7339701  0.57222249 0.45404761\n",
      "  0.82748498 0.9346387  0.55108455 0.39655797 0.28120388 0.82140318\n",
      "  0.96895867 0.32401541 0.62388023 0.39291907 0.62650463 0.47740097\n",
      "  0.40236052 0.35149774 0.45007009 0.54872334 0.70022465 0.41875876\n",
      "  0.65079983 0.76042317 0.59795689 0.7692659  0.52985678 0.22090379\n",
      "  0.67423273 0.0698249  0.25262534 0.60769265 0.01057634 0.87623872\n",
      "  0.56766128 0.72567256 0.31389935 0.4861733  0.68630576 0.49879295\n",
      "  0.37275899 0.69047704]]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 15 #размерность векторного представления фонем, то есть фонем из 50 элементов\n",
    "embed = (np.random.rand(len(vocab),embed_size) + 0.005) #векторов представлены случайными числами в диапазоне от 0 до 1 с плавающей точкой\n",
    "recurrent = np.eye(embed_size) #рекуррентная матрица (первоначально единичная)\n",
    "start = np.zeros(embed_size) #начальное векторное представление фонемы именно так начинается моделирование предлопредсказаний в нейронных сетях\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) + 0.005) #создается весовая матрица decoder для преобразования векторного представления фонемы в вектор вероятностей\n",
    "one_hot = np.eye(len(vocab)) #вспомогательная (единичная) матрица для расчета функции потерь\n",
    "\n",
    "print('Вектор эмбейдинга:' + '\\n', embed[:5])\n",
    "print('Матрица декодера:'  + '\\n', decoder[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    '''Функция принимает на вход список индексов фонем (слова)'''    \n",
    "    layers = list() #Список с именем layers для прямого распространения\n",
    "    layer = {} #словарб для слоёв\n",
    "    layer['hidden'] = start #скрытое состояние первого слоя начальным вектором 'start'\n",
    "    layers.append(layer) #добавляем первый слой в список\n",
    "    loss = 0 #для хранения суммарной ошибки\n",
    "\n",
    "    # итерация по слову\n",
    "    for target_i in range(len(word)): #проходим по всем фонемам в слове\n",
    "        layer = {} #словарь для этого слоя \n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) #извлекает вероятность, которую сеть предсказала для правильного следующего слова  \n",
    "\n",
    "        # вычисляем ошибку для текущего предсказания, используя отрицательный логарифм вероятности правильного слова, добавляем ошибку к общей суммарной ошибке 'loss'\n",
    "        epsilon = 1e-15 #добавляем маленькое число, которое используется для предотвращения взятия логарифма от нуля\n",
    "        loss += -np.log(layer['pred'][word[target_i]] + epsilon) #чем ниже вероятность предсказанного слова, тем выше значение loss\n",
    "        # выисляем скрытое состояние для следующего слоя, используя рекуррентную связь и эмбеддинги текущего слова\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[word[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss #возвращаем список слоев 'layers' и суммарную ошибку 'loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели по итерациям "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 0 64.20848959499746\n",
      "Perplexity: 10000 35.622687578403045\n",
      "Perplexity: 20000 41.58681125677364\n",
      "Perplexity: 30000 51.72054945250671\n",
      "Perplexity: 40000 41.42629307153692\n",
      "Perplexity: 50000 64.7735207636436\n",
      "Perplexity: 60000 34.41994125714539\n",
      "Perplexity: 70000 27.494414155269908\n",
      "Perplexity: 80000 27.27657285561164\n",
      "Perplexity: 90000 61.002870582512124\n",
      "Perplexity: 100000 27.77220923832629\n",
      "Perplexity: 110000 37.719943030984595\n",
      "Perplexity: 120000 44.651219212438725\n",
      "Perplexity: 130000 38.69872645664222\n",
      "Perplexity: 140000 31.806734745628912\n",
      "Perplexity: 150000 32.1935125889175\n",
      "Perplexity: 160000 21.874679964333303\n",
      "Perplexity: 170000 31.841787596784613\n",
      "Perplexity: 180000 47.786300606911624\n",
      "Perplexity: 190000 50.49641032155704\n",
      "Perplexity: 200000 31.49415766486017\n",
      "Perplexity: 210000 70.7798033631651\n",
      "Perplexity: 220000 44.46746652470285\n",
      "Perplexity: 230000 32.37675936483693\n",
      "Perplexity: 240000 26.988021600040074\n",
      "Perplexity: 250000 31.57058102123863\n",
      "Perplexity: 260000 49.75325840843228\n",
      "Perplexity: 270000 51.322381764578914\n",
      "Perplexity: 280000 41.51279587393894\n",
      "Perplexity: 290000 29.454771731759976\n",
      "Perplexity: 300000 33.411505593446634\n",
      "Perplexity: 310000 45.99937783478723\n",
      "Perplexity: 320000 27.82409206288016\n",
      "Perplexity: 330000 33.87602884244458\n",
      "Perplexity: 340000 70.77533225661045\n",
      "Perplexity: 350000 37.626195926123216\n",
      "Perplexity: 360000 18.11583879152645\n",
      "Perplexity: 370000 29.3881285454503\n",
      "Perplexity: 380000 41.563490568884134\n",
      "Perplexity: 390000 32.06229489102989\n",
      "Perplexity: 400000 37.34275284567801\n",
      "Perplexity: 410000 39.95159819284001\n",
      "Perplexity: 420000 66.11900386752937\n",
      "Perplexity: 430000 37.60369717803643\n",
      "Perplexity: 440000 30.826331875655217\n",
      "Perplexity: 450000 32.6230152765138\n",
      "Perplexity: 460000 53.02220458811646\n",
      "Perplexity: 470000 19.51648421631209\n",
      "Perplexity: 480000 61.400766029218765\n",
      "Perplexity: 490000 36.50076083766075\n"
     ]
    }
   ],
   "source": [
    "for iter in range(500000):\n",
    "    # прямое распространение\n",
    "    alpha = 0.000001 #сокрость обучения\n",
    "    phon_tokens = tokens[iter%len(tokens)]\n",
    "    phon_tokens = [token for token in phon_tokens if token] #отфильтровываем пустые строки\n",
    "    word = phons2indices(phon_tokens) #выбираем случайное предложение из списка токенов и преобразуем его в список индексов\n",
    "    layers,loss = predict(word) #получаем список слоев и ошибку\n",
    "\n",
    "    # обратное распространение для обновления весов \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx] \n",
    "        target = word[layer_idx-1]\n",
    "        # проверяем, что слой не первый \n",
    "        if(layer_idx > 0): \n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target] #вычисляем разницу между предсказанным и ожидаемым вектором\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose()) #вычисляем ошибку скрытого состояния текущего слоя\n",
    "\n",
    "            # проверяем, что слой не последний\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                # ошибка скрытого состояния вычисляется на основе ошибки текущего слоя и ошибки скрытого состояния следующего слоя\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else: #если слой первый \n",
    "            # вычисляем ошибку скрытого состояния первого слоя на основе ошибки скрытого состояния второго слоя\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "\n",
    "    # теперь корректируем веса\n",
    "    # обновляем начальное скрытое состояние на основе ошибки скрытого состояния первого слоя, деля на длину предложения для нормализации\n",
    "    start -= layers[0]['hidden_delta'] * alpha / (len(word))\n",
    "    # итерация по всем слоям со второго \n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "        # корректируем матрицу декодера с учетом скрытого состояния текущего слоя и ошибки выходного слоя, деля на длину предложения для нормализации\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / (len(word))\n",
    "        \n",
    "        # извлекаем индекс текущего слова из списка индексов предложения\n",
    "        embed_idx = word[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / (len(word)) #корректируем вектор эмбеддинга текущего слова с помощью ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / (len(word)) #обновляем матрицу рекуррентной связи с помощью скрытого состояния текущего слоя и ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "    \n",
    "    # выводим перплексию каждые 1000 итераций \n",
    "    if(iter % 10000 == 0):\n",
    "        if str(np.exp(loss/len(word))) == 'nan':\n",
    "               break\n",
    "        else:\n",
    "            print(\"Perplexity:\", iter, str(np.exp(loss/len(word))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попросим сеть предсказать следующую фонему в слове с индексом n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'a', 't', \"v'\", 'e', 'r', \"d'\", 'i', 'l', 'a', \"s'\"]\n",
      "Prev Input:p           True:a              Pred:a\n",
      "Prev Input:a           True:t              Pred:e\n",
      "Prev Input:t           True:v'             Pred:e\n",
      "Prev Input:v'          True:e              Pred:l\n",
      "Prev Input:e           True:r              Pred:a\n",
      "Prev Input:r           True:d'             Pred:a\n",
      "Prev Input:d'          True:i              Pred:a\n",
      "Prev Input:i           True:l              Pred:a\n",
      "Prev Input:l           True:a              Pred:a\n",
      "Prev Input:a           True:s'             Pred:a\n"
     ]
    }
   ],
   "source": [
    "word_index = 25\n",
    "\n",
    "#  l - список слоев, _ - игнорируемая переменная loss\n",
    "l,_ = predict(phons2indices(tokens[word_index]))\n",
    "# выводим предложение с индексом 10 в виде списка\n",
    "print(tokens[word_index])\n",
    "# итерация по слоям кроме первого и последнего\n",
    "for i,each_layer in enumerate(l[1:-1]):\n",
    "    # извлекаем слово из предложения\n",
    "    input = tokens[word_index][i]\n",
    "    # извлекаем оригинальное следующее слово предложения\n",
    "    true = tokens[word_index][i+1]\n",
    "    # предсказанное следующее слово\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    # выводим результат\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
    "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'a', \"l'\", 'i', 'zh', 'e', 'l', 'a']\n",
      "Prev Input:p           True:r              Pred:a\n",
      "Prev Input:r           True:a              Pred:e\n",
      "Prev Input:a           True:l'             Pred:e\n",
      "Prev Input:l'          True:i              Pred:e\n",
      "Prev Input:i           True:zh             Pred:a\n",
      "Prev Input:zh          True:e              Pred:a\n",
      "Prev Input:e           True:l              Pred:a\n",
      "Prev Input:l           True:a              Pred:i\n"
     ]
    }
   ],
   "source": [
    "word_index = 15\n",
    "\n",
    "#  l - список слоев, _ - игнорируемая переменная loss\n",
    "l,_ = predict(phons2indices(tokens[word_index]))\n",
    "# выводим предложение с индексом 10 в виде списка\n",
    "print(tokens[word_index])\n",
    "# итерация по слоям кроме первого и последнего\n",
    "for i,each_layer in enumerate(l[1:-1]):\n",
    "    # извлекаем слово из предложения\n",
    "    input = tokens[word_index][i]\n",
    "    # извлекаем оригинальное следующее слово предложения\n",
    "    true = tokens[word_index][i+1]\n",
    "    # предсказанное следующее слово\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    # выводим результат\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
    "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как показывает результат сеть чаще предсказывает гласную /a/, либо любую другую гласную, так как они чаще встречаются в обучающем наборе"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
