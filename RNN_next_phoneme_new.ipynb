{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение рекуррентной нейронной сети для предсказания следующей фонемы\n",
    "## В данном коде мы будем обучать базовую рекуррентную нейронную сеть с прямым и обратным распространением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import sys\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['j', 'u', \"r'\", 'i'], ['t', \"r'\", 'i', 'f', 'a', 'n', 'a', 'f'], ['a', \"b'\", \"m'\", 'e', 'n'], ['y', 'j', 'u', \"l'\", 'e'], ['m', 'a', 'c']]\n",
      "75235\n"
     ]
    }
   ],
   "source": [
    "# чтение датасета, состоящего из фонетически транскрибированных слов (каждая фонема через пробел)\n",
    "f = open('phon.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# создание пустого списка для токенов\n",
    "tokens = list()\n",
    "for line in raw[0:]:  \n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[0:]) #добавление слов в список токенов\n",
    "\n",
    "# таким образом токены состоят из списков, содержащих слово в транскрипции \n",
    "print(tokens[0:5]) #вывод первых пяти элементов (токенов)\n",
    "print(len(tokens)) #длина списка токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формируем список существующих элементов из токенов и присваиваем им уникальный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"n'\", 'zh', \"j'\", 'm', \"v'\", \"f'\", 'd', \"t'\", 't', 'j', \"l'\", 'v', \"c'\", 'e', \"sh'\", \"zh'\", 'r', 'sch', 'k', \"p'\", 'y', \"r'\", 'b', 'g', 's', 'i', 'c', 'l', 'p', \"g'\", 'sh', 'ch_', \"s'\", 'o', \"d'\", 'dzh', 'sc', 'n', \"k'\", 'ch', \"m'\", 'h', 'u', 'f', 'a', \"dz'\", \"b'\", \"z'\", \"h'\", 'z']\n",
      "50\n",
      "{\"n'\": 0, 'zh': 1, \"j'\": 2, 'm': 3, \"v'\": 4, \"f'\": 5, 'd': 6, \"t'\": 7, 't': 8, 'j': 9, \"l'\": 10, 'v': 11, \"c'\": 12, 'e': 13, \"sh'\": 14, \"zh'\": 15, 'r': 16, 'sch': 17, 'k': 18, \"p'\": 19, 'y': 20, \"r'\": 21, 'b': 22, 'g': 23, 's': 24, 'i': 25, 'c': 26, 'l': 27, 'p': 28, \"g'\": 29, 'sh': 30, 'ch_': 31, \"s'\": 32, 'o': 33, \"d'\": 34, 'dzh': 35, 'sc': 36, 'n': 37, \"k'\": 38, 'ch': 39, \"m'\": 40, 'h': 41, 'u': 42, 'f': 43, 'a': 44, \"dz'\": 45, \"b'\": 46, \"z'\": 47, \"h'\": 48, 'z': 49}\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "vocab = set() #создается простой список фонем из словаря\n",
    "for sound in tokens: #итерация по каждому элементу в списке 'tokens'\n",
    "    for phon in sound: #теперь по каждому звуку\n",
    "        if phon == '': #проверям не пуста ли фонема, если да пропускаем\n",
    "            pass\n",
    "        else:\n",
    "            vocab.add(phon) #добавляем фонему в 'vocab', если фонема уже есть, добавление не происходит\n",
    "vocab = list(vocab) #преобразование множества в список\n",
    "phon2index = {} #создание пустого словаря для хранения фонемы и ее индекса\n",
    "for i,phon in enumerate(vocab):  #итерация по списку 'vocab', получая индекс i и фонему 'phon' \n",
    "    phon2index[phon]=i\n",
    "    \n",
    "def phons2indices(word, phon2index):\n",
    "    '''Функция для преобразования списка Фонем в список индексов'''\n",
    "    idx = list() #список для индексов\n",
    "    for phon in word: #проходим по фонемам в слове\n",
    "        if phon == '': #проверяем на пустую строчку\n",
    "            pass\n",
    "        else:\n",
    "            idx.append(phon2index[phon]) #ищем индекс фонемы в словаре 'phon2index' и добавляет его в список 'idx'\n",
    "    return idx\n",
    "\n",
    "def softmax(x): \n",
    "    '''Функция активации softmax для предсказания следующей фонемы'''\n",
    "    e_x = np.exp(x - np.max(x)) #ищем экспоненту каждого элемента вектора, вычитая максимальное значение \n",
    "    return e_x / e_x.sum(axis=0) #нормализуем экспоненциальные значения, деля на их сумму\n",
    "    # возвращает вектор вероятностей\n",
    "\n",
    "# выводим наш список vocab со всеми упомянутыми фонемами\n",
    "print(vocab)\n",
    "# длина списка vocab\n",
    "print(len(vocab))\n",
    "# выводим словарь, где каждой фонеме присвоен уникальный индекс (начиная с 0)\n",
    "print(phon2index)\n",
    "# длина словаря\n",
    "print(len(phon2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделение данных на обучающую и тестовую выборки 80/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(tokens) * 0.8)\n",
    "train_tokens = tokens[:split_index]\n",
    "random.shuffle(train_tokens)\n",
    "test_tokens = tokens[split_index:]\n",
    "random.shuffle(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['p', 'a', \"n'\", 'i', 'm', 'a', 'e', 'sh'], ['s', 'g', 'u', 'sc', 'a', 'l', 'y', \"s'\"], [\"d'\", 'i', 'v', 'a', 'n', 'a'], ['k', 'a', 'z', 'a', 'l'], ['h', 'a', 'l', 'a', \"d'\", 'i', \"l'\", \"n'\", 'i', 'k'], [\"s'\", 'e', 'k', 'u', 'n', 'd', 'u'], ['k', \"r'\", 'i', 'm', \"l'\", 'o', 'f', 's', \"k'\", 'i', 'i'], ['p', 'a', 'l', 'u', 'ch', 'i', 't'], [\"l'\", 'e', 't', 'a', 'm'], ['m', 'o', 'sh', 'y']]\n",
      "[['s', 't', 'r', 'a', 'n', 'y', 'm'], ['e', 'i', 'm', 'u'], ['v', 'y', \"t'\", 'a', \"g'\", 'i', 'v', 'a', 'e'], ['p', 'a', 'd', 'n', 'o', 's'], ['n', 'a', 'sh'], [\"n'\", 'e', 'c'], ['r', 'y', 'z', 'v', 'a', 'r', 'a', 'ch', 'i', 'v', \"t'\"], ['sh', 't'], [\"n'\", 'e', 'o', ''], [\"v'\", 'e', 'r', 'n', 'a']]\n"
     ]
    }
   ],
   "source": [
    "# выводим первые 10 токенов обучающей и тестовой выборки \n",
    "print(train_tokens[:10])\n",
    "print(test_tokens[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задаем параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 12 #размерность векторного представления фонем, то есть фонем из 50 элементов\n",
    "learning_rate = 0.00001 #устанавливаем скорость обучения\n",
    "BATCH_SIZE = 32 #размер батча\n",
    "EPOCHS = 100 #количество эпох\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция для создания батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(tokens, batch_size, phons2indices):\n",
    "    '''Функция принимает список токенов (слов) и размер батча'''\n",
    "    batches = [] #пустой список для батчей\n",
    "    for i in range(0, len(tokens), batch_size): #итерирация по списку токенов с шагом, равным размеру батча\n",
    "        batch = tokens[i:i + batch_size] #извлекаем из списка токена батч\n",
    "        # фильтруем пустые слова и преобразуем в индексы\n",
    "        batch = [[phons2indices[token] for token in word if token] for word in batch if word] # вложенный генератор списка\n",
    "        # фильтруем пустые списки (слова)\n",
    "        batch = [word for word in batch if word]\n",
    "        batches.append(batch) #добавляем батч\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрицы для предсказания\n",
    "# Инициализируем веса с Xavier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вектор эмбейдинга: [[ 0.22217504  0.03158586  0.09179856 -0.1710558   0.12330979  0.01854156\n",
      "   0.27043926  0.08162412 -0.24522745 -0.01043713  0.03181435 -0.13991637]\n",
      " [-0.16771883 -0.01625454 -0.06915435  0.03534646  0.16146157 -0.0330332\n",
      "   0.16403574  0.05446619 -0.33541509 -0.19279283 -0.05290679 -0.03043818]\n",
      " [ 0.22302484 -0.31246977  0.06511925  0.12313562 -0.06024685 -0.24229842\n",
      "   0.12216036  0.0618876  -0.1904398  -0.22902078 -0.18409679  0.10996646]\n",
      " [-0.03284311  0.16854042 -0.06562262 -0.0682751  -0.10141138 -0.11307056\n",
      "  -0.19348044  0.11801743  0.15502453 -0.28761456  0.16508476 -0.13073577]\n",
      " [ 0.08022746  0.05130583 -0.08224335 -0.14908136 -0.11711028  0.22144012\n",
      "   0.06968157 -0.19630513  0.12697555  0.21827534 -0.10495836 -0.02214608]\n",
      " [-0.165051    0.16314869  0.05663299  0.07092673  0.02366165  0.12248547\n",
      "  -0.06930872 -0.02546446 -0.19903565  0.13631828  0.09523311  0.02455648]\n",
      " [-0.04008526  0.10813775  0.01142908  0.10996195 -0.03532518  0.06647279\n",
      "   0.2011956  -0.22322829 -0.23625694 -0.02824441  0.16020144 -0.10915658]\n",
      " [ 0.1307869  -0.31817591 -0.00053476 -0.02467671 -0.18404381  0.07102861\n",
      "   0.30536409  0.01018623 -0.18411411  0.02738062  0.30415063 -0.20973455]\n",
      " [ 0.04993173 -0.05656264  0.10062429 -0.14047772 -0.04698618  0.17809618\n",
      "  -0.3678323  -0.05947761 -0.02421059 -0.1244551  -0.18707434 -0.03578993]\n",
      " [ 0.260956   -0.18682837  0.0208135   0.14599182 -0.14560296 -0.11618857\n",
      "  -0.05216017 -0.25974987  0.42929828 -0.02623983  0.01342938  0.19203182]\n",
      " [ 0.05770253  0.06480634 -0.02476222 -0.07090626  0.03602169 -0.14900202\n",
      "   0.04131679  0.06651415 -0.09445599 -0.22429255 -0.27827968  0.02150148]\n",
      " [ 0.07655601  0.0939818  -0.15012678 -0.12381569 -0.00492961  0.09422341\n",
      "   0.21147238  0.04416544 -0.05129812 -0.00504041 -0.01674235  0.16875316]\n",
      " [ 0.24609362 -0.42564402  0.20195007  0.03517877 -0.11312256 -0.01756152\n",
      "   0.00866594 -0.15922729 -0.18154449 -0.10438486  0.17109454  0.02025807]\n",
      " [-0.05517655 -0.13251359  0.02893467  0.09537588 -0.12619291  0.18966437\n",
      "  -0.03626938  0.23398416 -0.04958953  0.04922675  0.10299393  0.03172881]\n",
      " [-0.02287712 -0.23180618  0.03353348 -0.0757141   0.16563268  0.14244723\n",
      "  -0.09008615  0.02723338  0.02987571 -0.11876877  0.02275012 -0.15930515]\n",
      " [ 0.22057648  0.06250061  0.12640123 -0.04749342  0.02344041  0.11451717\n",
      "   0.16833224 -0.03280429 -0.21738577  0.26067036  0.29937691 -0.05552315]\n",
      " [ 0.07960529 -0.00925852 -0.00366508  0.01308536  0.11252095  0.01053783\n",
      "   0.34823353 -0.00706663  0.02565261 -0.02104449  0.26327056 -0.01160525]\n",
      " [-0.08085207  0.22745003  0.09725765 -0.21142378 -0.04208509  0.01252114\n",
      "   0.135491    0.0289249  -0.07271901  0.1990858   0.029996    0.15070279]\n",
      " [-0.1068158  -0.02211025  0.01773851 -0.07841746 -0.22438163  0.03487758\n",
      "  -0.10910219  0.08396364  0.03687297  0.29574784  0.15528735  0.12383367]\n",
      " [ 0.01032111  0.08467815  0.01536782  0.02954528 -0.12279028 -0.09752201\n",
      "   0.1380665  -0.01091323  0.03845297 -0.25140425 -0.06913407  0.27064754]\n",
      " [-0.04536615  0.09807549 -0.07691607  0.0201441   0.06925476 -0.01876973\n",
      "   0.01162893 -0.05163352  0.20257227 -0.13873838 -0.23937052  0.01205101]\n",
      " [ 0.23680267  0.03456636 -0.07020565 -0.08997012  0.02331045  0.0153051\n",
      "  -0.26041139 -0.22456863  0.12942191  0.25987876  0.27630578  0.02753603]\n",
      " [ 0.0842424  -0.21975457 -0.17986759 -0.06502071  0.01930699 -0.08669675\n",
      "  -0.06452364 -0.09162486  0.04659592  0.15190179  0.09762307  0.09596538]\n",
      " [-0.27135531  0.08536778 -0.15342684  0.027354    0.00269026  0.19271606\n",
      "   0.09346496 -0.16838288 -0.04020506  0.01132003 -0.11647439 -0.10654002]\n",
      " [-0.20702289  0.1862681   0.17262772 -0.0734529  -0.00276749 -0.04868381\n",
      "   0.11657706 -0.23492082 -0.1268755   0.02162485  0.00527432  0.04386215]\n",
      " [ 0.14837318  0.1795383  -0.10237565 -0.16562481 -0.0540998  -0.06276709\n",
      "   0.15132124  0.01426596  0.10922406 -0.03400149  0.06189674 -0.20299626]\n",
      " [ 0.14595613  0.22904364  0.0937631  -0.08497616  0.19606082  0.0467753\n",
      "   0.08877134  0.08400161  0.16387023 -0.25270206  0.16276408 -0.05797161]\n",
      " [ 0.03616875 -0.14767665 -0.10753918 -0.26471942 -0.17542962 -0.09199351\n",
      "  -0.1855938  -0.06040433  0.03398815 -0.02476037  0.00306729  0.11704249]\n",
      " [-0.14294737 -0.01581127 -0.16832066  0.14992561  0.17090708 -0.09259492\n",
      "   0.06852132  0.07538305 -0.0159985  -0.10732543 -0.19209696 -0.21991764]\n",
      " [-0.09057328  0.23224613 -0.11770717  0.00473097  0.1362158   0.04799442\n",
      "  -0.0453369  -0.03832186 -0.31284493 -0.24016959  0.17846896  0.09988722]\n",
      " [-0.17473957  0.03915833 -0.08125496  0.04744609 -0.03167043 -0.15913406\n",
      "   0.10124907  0.00677224  0.09660845 -0.10933941  0.04784063 -0.12918439]\n",
      " [-0.08639376 -0.10092748 -0.09958512  0.24246609 -0.0030042   0.05064615\n",
      "   0.09451916 -0.12300964 -0.01774121  0.01954513  0.16386704 -0.09865333]\n",
      " [-0.01718222  0.10895874  0.14665066  0.16474637  0.03016522 -0.10954786\n",
      "  -0.06582802  0.05388151  0.04544594  0.12491534  0.02264534 -0.1867088 ]\n",
      " [ 0.00132075  0.10143261  0.15224504 -0.11170138 -0.13694425  0.05667864\n",
      "  -0.00996356  0.15056382  0.08370547  0.15841589 -0.05052616  0.11842758]\n",
      " [ 0.04064997  0.25849735 -0.10016044  0.08396316 -0.00768295  0.12403925\n",
      "  -0.15447788 -0.13855059 -0.12498064  0.16136192  0.02631386  0.24284243]\n",
      " [ 0.05310426  0.08422276  0.10984618  0.09052999 -0.07036429 -0.17506469\n",
      "   0.0905405   0.04012576 -0.09204811  0.07546358 -0.24413164 -0.14841598]\n",
      " [-0.07305875  0.0915177   0.35303723 -0.10657955  0.03114698 -0.14858484\n",
      "  -0.14161641 -0.04264254  0.0077444   0.07710345 -0.02173407 -0.02379801]\n",
      " [ 0.02122614 -0.05113622  0.17147608  0.12182781 -0.12169707  0.12372426\n",
      "   0.1311879  -0.06793243 -0.04094737 -0.28031335 -0.05085772  0.00529618]\n",
      " [-0.19759458 -0.3540816   0.05328194 -0.07305492  0.10567818 -0.12127672\n",
      "   0.06853025 -0.1770979  -0.09589534  0.13298164  0.0858456  -0.3768387 ]\n",
      " [-0.05546976 -0.02420168  0.25698026  0.09763436  0.08206061 -0.01689997\n",
      "  -0.25141063  0.14467861  0.10553297 -0.05063133  0.03721578 -0.12688949]\n",
      " [-0.01937004  0.01405298  0.34045767 -0.01240013 -0.12891554  0.04348262\n",
      "   0.1240689   0.04261236 -0.09494737 -0.11144953 -0.08477101  0.10693889]\n",
      " [-0.01522483 -0.0566243  -0.00973415  0.0072196   0.04031206  0.18584229\n",
      "  -0.04266932  0.09432142  0.1792046   0.06415486  0.14492135  0.18119128]\n",
      " [-0.12872714  0.06343181  0.08806951  0.33477204  0.18267406  0.13832\n",
      "  -0.05249081  0.18945179  0.09527828 -0.13116979  0.00556121  0.13024309]\n",
      " [-0.06813665 -0.09652549 -0.20100426 -0.14301253  0.03306461 -0.03238009\n",
      "   0.05324114  0.09133512  0.21083446  0.12518343  0.12931126 -0.02036718]\n",
      " [-0.09928785 -0.08370886  0.27408174  0.0355721   0.04106461  0.00962831\n",
      "   0.1279925  -0.02278892  0.17732635 -0.02267224  0.01423439 -0.09266647]\n",
      " [ 0.0015024   0.22901504 -0.00254141 -0.1375075  -0.06843417 -0.19610586\n",
      "   0.05509216 -0.00271524  0.11908137 -0.10461029  0.00427115 -0.03096912]\n",
      " [-0.08045414  0.28138081  0.07846037  0.22620524 -0.1874122  -0.08480764\n",
      "   0.09257478  0.13574444 -0.03950066  0.04391337 -0.03554581 -0.10111232]\n",
      " [-0.19612435 -0.07561252  0.11061752  0.05111381  0.16510897 -0.07723107\n",
      "  -0.02254638 -0.09562625  0.20484092 -0.07100839  0.01553907  0.10929442]\n",
      " [-0.08672288  0.14558099 -0.16667321  0.02352029 -0.05795869  0.07742424\n",
      "   0.1334406  -0.23011142  0.26568919 -0.1675322   0.1362298   0.1413634 ]\n",
      " [-0.02384013 -0.15699079  0.10272139  0.16336363 -0.07041272 -0.25534537\n",
      "   0.25287205  0.22309814  0.0689382  -0.07067764 -0.06176079 -0.0548882 ]]\n",
      "Матрица декодера: [[-1.69807996e-01 -5.91152990e-03 -2.91716818e-01  1.22651095e-01\n",
      "   3.12859076e-01  2.06318013e-01 -4.07558844e-02  1.26599346e-01\n",
      "   1.00235465e-01  2.49041842e-01 -3.94761847e-01  1.77987307e-02\n",
      "   1.47539539e-01 -1.81578895e-01 -5.64329263e-01 -4.24541856e-02\n",
      "   1.03691609e-02 -8.01336624e-02  2.51875015e-01 -4.32803921e-03\n",
      "  -2.82449400e-01  1.99979150e-01  2.17725121e-02 -3.69307963e-02\n",
      "   2.83616360e-02 -1.87692829e-01 -7.25605745e-01 -3.28336991e-01\n",
      "  -1.18175511e-01 -1.98111484e-01  3.69471270e-02  1.89486915e-01\n",
      "  -2.22375660e-01  1.71232315e-01 -7.08724364e-02 -3.07497550e-01\n",
      "  -2.67362852e-01 -2.69894971e-01  2.46542627e-01  1.70979222e-01\n",
      "   1.12791543e-01  1.63648465e-01 -2.35045493e-01  1.31730762e-01\n",
      "  -1.43486330e-01  2.74371157e-01  9.45087083e-02 -9.92349304e-02\n",
      "   2.86841521e-01 -1.07890749e-01]\n",
      " [-4.02504254e-01  3.32191704e-02  3.73327477e-01  2.02349618e-01\n",
      "   1.89290633e-01  1.91910083e-02 -1.50553070e-01 -3.86181192e-01\n",
      "   1.65748284e-01  2.68518983e-01 -3.50411029e-01 -1.82232665e-01\n",
      "   2.03606434e-01 -3.29869036e-01 -2.33258934e-01  1.10060222e-01\n",
      "   2.52535705e-01 -5.60243609e-02  2.57989489e-01  5.70201929e-01\n",
      "   4.79968483e-01 -4.50647253e-02  7.12532341e-02  3.38215941e-01\n",
      "  -4.90679444e-02 -4.03689347e-01 -5.19939895e-01  6.14901763e-02\n",
      "  -8.05320019e-02 -1.20283005e-01  5.98953153e-03 -5.19025150e-01\n",
      "   2.96886222e-01  8.07352684e-02 -1.17455269e-01  2.28401841e-01\n",
      "  -2.59889894e-01 -8.43016131e-02 -2.73834566e-02 -2.76426082e-01\n",
      "  -5.79919863e-02 -4.67291024e-01  2.65137712e-01 -1.99988491e-02\n",
      "  -1.09896122e-01  4.39748747e-02  2.71827617e-01  4.67128269e-01\n",
      "  -2.55786201e-01  1.42995285e-01]\n",
      " [ 5.16128287e-01 -1.13332075e-01 -4.84536226e-01 -1.51105355e-01\n",
      "   5.64261194e-01  1.60088668e-01  3.37811555e-01  1.93121303e-01\n",
      "   1.57488431e-01 -2.61337635e-01  2.01332137e-01 -5.16459186e-01\n",
      "  -7.08693650e-01 -7.31740852e-01 -1.23507652e-01  7.81394370e-02\n",
      "  -2.43980144e-01 -2.46477087e-01  5.05161400e-01 -5.37295145e-01\n",
      "  -1.67817097e-01  1.14734523e-01 -4.03078797e-01  8.88281188e-02\n",
      "  -4.30336243e-02  4.27575929e-01 -2.00035766e-01 -4.31448143e-02\n",
      "  -2.17103234e-01 -2.94022621e-01 -8.63407223e-02 -3.14040074e-01\n",
      "   1.59254187e-01 -1.48622221e-01  1.09488258e-01  5.84901449e-01\n",
      "  -6.05108382e-03  1.27308228e-02  1.16641360e-01  2.41895603e-02\n",
      "   1.85204512e-01 -3.22534628e-01  5.54984299e-01  4.82564091e-02\n",
      "  -1.54882868e-01 -3.49825742e-01 -6.03605451e-01  2.49868164e-02\n",
      "   2.38438703e-01  2.84178871e-01]\n",
      " [-1.04789962e-01  3.01779876e-01 -3.60806129e-01  7.00296555e-01\n",
      "  -3.67082019e-01  5.56220461e-02 -4.25721105e-02  5.52596298e-02\n",
      "  -8.03562376e-03 -1.92483369e-01 -2.83157333e-01 -2.04423644e-01\n",
      "   1.38382407e-01  3.29517960e-01 -1.93977076e-01 -1.11672938e-01\n",
      "  -7.28768028e-03 -9.26968283e-02 -1.34119228e-01  7.95729859e-02\n",
      "   4.59856280e-01  9.72840027e-02  1.25898481e-01  2.30874058e-01\n",
      "   2.69609774e-01  4.00853004e-01  5.95061410e-01 -1.00964295e-01\n",
      "  -6.62323255e-06 -7.82162390e-02 -3.27215671e-01 -1.81497239e-01\n",
      "   4.14992190e-01  4.30738659e-01  7.54023977e-02 -1.41926931e-01\n",
      "   1.37778685e-01 -1.50079643e-02  5.00855078e-02  1.28769011e-01\n",
      "   1.66979103e-02 -3.80096041e-02 -2.90149960e-01  1.70217258e-01\n",
      "   5.68519251e-01  4.62374750e-01  1.08383575e-01  1.15902797e-01\n",
      "   5.00343610e-02 -5.50655351e-02]\n",
      " [-2.54619493e-01  2.64807224e-01  2.76965358e-02 -2.93047090e-01\n",
      "   8.48809233e-02  3.64388912e-01  4.45463521e-01  4.40150193e-01\n",
      "   1.60215740e-01  3.40519065e-01  1.61966683e-01 -1.50264337e-01\n",
      "  -2.18535396e-01 -2.73239297e-01  2.91201810e-01  7.54846782e-02\n",
      "   4.38017118e-02 -1.66239369e-02  1.70076211e-01 -2.10149570e-01\n",
      "   2.60809289e-01 -3.86139261e-01 -4.31523533e-02  2.81826661e-01\n",
      "   5.93727907e-02  4.79420078e-01 -4.22230404e-01 -1.81443683e-01\n",
      "  -1.68378959e-01  3.56704846e-02 -3.56123720e-01 -3.54570073e-01\n",
      "  -2.92869586e-01  3.90881139e-01  6.11463943e-01  1.66827312e-02\n",
      "   1.28730648e-01 -3.09925358e-02  1.95193527e-01 -1.94452173e-01\n",
      "   2.20304969e-01  4.68747090e-02 -1.31334972e-01  6.98183200e-01\n",
      "  -3.04763298e-01  3.89209124e-03 -1.44125731e-01  3.42849465e-01\n",
      "  -4.31009146e-01 -8.87651381e-02]\n",
      " [ 1.93626195e-01 -4.29180752e-02  1.54368736e-01  2.15203362e-02\n",
      "   1.86002009e-02 -3.54700585e-02  5.88802920e-01 -1.32405028e-01\n",
      "   2.50072754e-01 -4.11101339e-01 -2.68758501e-01 -1.38456267e-01\n",
      "   3.54035778e-01  1.66383258e-01 -4.59900389e-01  2.94560398e-01\n",
      "  -5.58859869e-01 -1.15768612e-01 -3.21303402e-01  7.90774975e-02\n",
      "   3.98694232e-01 -1.93739890e-01 -1.12611052e-01 -1.94991551e-01\n",
      "  -1.59346052e-02  2.33926727e-01 -2.12192371e-01 -1.15438293e-04\n",
      "  -2.98558801e-01  4.47538263e-02 -4.26599291e-01  5.40976601e-02\n",
      "   2.37748676e-02 -7.88995087e-01  3.06699967e-01  3.04716960e-01\n",
      "   3.46195174e-01  4.29573127e-01  6.57706558e-03  2.69359011e-01\n",
      "  -8.02468627e-02 -4.34691300e-02  1.02128862e-01 -3.62791862e-01\n",
      "   1.83238242e-01  3.43698087e-02  3.98599046e-01 -1.54850616e-02\n",
      "  -3.20966542e-01  5.79813182e-01]\n",
      " [ 3.36114901e-01  1.65394874e-01 -5.86834400e-01 -6.44628941e-02\n",
      "   3.30893364e-01  3.13166393e-01  2.93590266e-01  2.45060146e-01\n",
      "  -1.31710806e-01 -1.94996120e-01  4.12697820e-01 -2.73407637e-01\n",
      "   6.94105648e-01 -4.36775392e-02  4.22951098e-02  4.32763926e-01\n",
      "  -2.64694532e-01  6.52562468e-01 -5.25592337e-01  2.63726991e-02\n",
      "   5.17611040e-02  4.84442520e-01  1.63424453e-01  2.33611021e-01\n",
      "   2.86807988e-01 -4.00802433e-01 -1.75246556e-01  2.71282138e-01\n",
      "   4.24409025e-01  4.33560165e-01  6.94159860e-01  1.27980692e-01\n",
      "  -3.38255287e-02  2.29242589e-01  3.21428186e-01 -1.17855263e-01\n",
      "   2.65437302e-01  2.85365546e-01 -1.42904061e-01 -1.41052402e-01\n",
      "   6.76878691e-02 -6.84707000e-02 -4.06597987e-01 -1.47028622e-01\n",
      "   3.16389641e-01  2.57502528e-01  5.86000449e-01  2.78030751e-01\n",
      "   5.74866462e-01  4.05697942e-01]\n",
      " [-5.35894751e-01 -7.60658498e-02 -2.98327912e-01 -7.85213977e-02\n",
      "  -3.10269250e-02  1.75607776e-01 -1.32962541e-01  2.12502732e-01\n",
      "   1.85124868e-01  1.20376178e-01 -1.04831146e-01  3.29171118e-01\n",
      "  -4.52030584e-01 -6.65846919e-01  1.05115224e-01  2.96010586e-01\n",
      "   4.21411485e-01 -5.10014412e-01  4.10585781e-01 -2.70933794e-01\n",
      "  -3.30012866e-01 -1.57264002e-03  1.49555686e-02  4.56034982e-01\n",
      "   4.03486319e-01 -1.63253582e-01 -1.50963655e-01 -1.26857782e-01\n",
      "  -2.87238858e-02 -1.77218751e-01  1.72196472e-01 -1.06175452e-02\n",
      "   1.49602611e-01  1.18760897e-01  8.11949540e-02 -2.02487609e-01\n",
      "  -7.92279238e-02  2.04467841e-01  1.90593546e-01 -8.73837906e-01\n",
      "  -2.25427565e-01  1.35349762e-01  3.24507808e-01  8.15129945e-02\n",
      "  -1.86290526e-01  4.31453822e-01 -3.19614208e-02 -5.81909605e-01\n",
      "   3.37917049e-01 -4.29252494e-01]\n",
      " [-1.99763369e-01  1.93937711e-01 -1.74833865e-01  5.67542177e-01\n",
      "  -6.84041642e-01  2.05888819e-01 -2.03637890e-01 -5.69064536e-02\n",
      "   4.93663474e-02 -2.69445090e-01 -5.45057134e-02  4.03421142e-01\n",
      "   1.07582160e-01  4.92038695e-01  5.66334483e-01  2.00493728e-01\n",
      "  -4.97130170e-02 -1.78060794e-01 -4.37774688e-01 -4.61176817e-01\n",
      "   1.99365939e-01  1.30439178e-01 -3.26848189e-01  3.74259394e-02\n",
      "   1.05763290e-01 -1.52873564e-01  9.53096646e-02  7.68913065e-02\n",
      "  -2.61022898e-01  5.49028249e-02  2.47215391e-01 -1.18325199e-01\n",
      "   2.40500723e-01 -3.12295954e-01 -2.12162942e-01  5.17248644e-01\n",
      "   1.32456280e-01 -3.40273933e-01  4.12512976e-02  2.64676886e-01\n",
      "   1.41423437e-01 -3.22016279e-01 -1.32789470e-02  2.01276400e-01\n",
      "   1.31244649e-01 -3.18966713e-01  8.33748092e-02  9.70775449e-02\n",
      "   9.64675623e-02 -2.98548814e-01]\n",
      " [ 3.29473126e-01  2.16317525e-01 -4.49104043e-01  4.89633391e-02\n",
      "  -9.40649790e-02 -2.13209409e-01 -3.38976644e-01  5.27813022e-01\n",
      "   2.94279753e-01  1.50018376e-02 -1.06081299e-01  3.45619218e-02\n",
      "   1.38437378e-01 -3.71738626e-01  3.94909746e-02 -1.64647056e-01\n",
      "   3.98018968e-01  1.08482286e-01  5.93028636e-01  2.56845307e-01\n",
      "  -7.46125135e-01  4.31523485e-01 -7.27849419e-01  2.43958147e-02\n",
      "   2.09792438e-01 -6.15362206e-02  5.18446420e-01  1.43647356e-01\n",
      "  -5.18531979e-03  1.25085062e-01 -5.42614401e-04 -2.36214294e-01\n",
      "  -1.68193695e-01  2.52489204e-01 -1.10033075e-04  1.75389360e-01\n",
      "   2.58477256e-01  1.72122002e-01 -1.03248636e-01  3.54437474e-02\n",
      "   2.04864312e-01 -2.17259701e-01 -4.19214210e-01 -6.68466490e-02\n",
      "  -1.91957915e-01 -2.79903459e-01  7.89009634e-02  5.04656962e-01\n",
      "  -1.10059119e-01  6.40367236e-01]\n",
      " [ 8.98557968e-02  5.69158915e-02  3.49859271e-01 -1.33552832e-01\n",
      "   1.72040074e-01 -5.74389651e-01  2.87091579e-01  1.30155413e-02\n",
      "  -8.94626585e-02 -1.51678819e-01  9.17502590e-02 -3.27156353e-02\n",
      "  -4.20118389e-01  2.66882647e-01  5.40587487e-01 -3.81150920e-01\n",
      "   1.13951365e-01 -1.22892968e-01  2.38556008e-01 -3.28633941e-01\n",
      "   1.17943145e-01  6.69008697e-02 -1.82109423e-01  2.82790506e-01\n",
      "   2.30851842e-01 -1.12998455e-01 -2.99353675e-01 -2.67171804e-01\n",
      "  -2.58829755e-01 -4.75484373e-02  3.35329711e-01  1.63030059e-01\n",
      "   2.24500036e-01 -1.26512686e-01 -1.33492244e-01 -2.92980615e-01\n",
      "   2.00561466e-01  2.93216065e-01  1.82344856e-02 -1.02968701e-01\n",
      "   3.65356487e-01  4.66371781e-02  1.47892300e-01  1.10984242e-01\n",
      "   5.25133246e-01 -1.48544601e-01  2.60498966e-01 -2.22182923e-01\n",
      "  -4.26886283e-02 -2.33066269e-01]\n",
      " [ 2.39738920e-01 -1.33066901e-01 -9.48854416e-02 -2.18005848e-01\n",
      "   1.44257343e-01 -3.36883322e-02  4.97527366e-01 -3.48765684e-02\n",
      "  -3.23817704e-01 -5.58471356e-02  3.10707607e-01  1.59403470e-01\n",
      "  -2.87744295e-01 -2.73065330e-01  5.17692307e-01 -3.99217181e-01\n",
      "  -2.23932590e-01  1.71077655e-01 -4.97636592e-02  5.45111372e-02\n",
      "   3.98574709e-02  4.31327359e-01  1.06562072e-01 -6.27926030e-01\n",
      "   1.81510650e-01  2.24468671e-01 -7.97990627e-02 -7.50797928e-01\n",
      "   2.13240187e-01 -3.81986680e-01  2.21058284e-02 -4.35424379e-01\n",
      "   9.88386430e-02  3.93211092e-01  2.48712055e-02 -5.71368932e-01\n",
      "   1.81716423e-01 -1.87501000e-01 -2.63007546e-01 -1.12508634e-01\n",
      "  -8.15207493e-02 -4.10335501e-01 -3.08166639e-01 -1.00901330e-01\n",
      "  -4.06077263e-01  2.03097219e-01 -2.34817088e-01  4.20375262e-01\n",
      "   1.77401384e-02  4.24366595e-01]]\n"
     ]
    }
   ],
   "source": [
    "'''Создаем матрицу эмбеддингов (векторных представлений) для фонем. \n",
    "Инициализируем матрицу случайными значениями из нормального распределения со средним значением 0 и стандартным отклонением, \n",
    "рассчитанным на основе размера словаря фонем размер матрицы = (количество фонем в словаре, размер эмбеддинга)'''\n",
    "embed = np.random.normal(0, 1/np.sqrt(len(vocab)), size=(len(vocab), embed_size))\n",
    "\n",
    "'''Создаем рекуррентную матрицу, инициализируем матрицу случайными значениями из нормального распределения со средним значением 0\n",
    "и стандартным отклонением, рассчитанным на основе размера эмбеддинга размер матрицы = (размер эмбеддинга, размер эмбеддинга)'''\n",
    "recurrent = np.random.normal(0, 1/np.sqrt(embed_size), size=(embed_size, embed_size))\n",
    "\n",
    "'''Создаем начальный вектор скрытого состояния и инициализируем вектор нулями размер вектора = (размер эмбеддинга)'''\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "'''Создаем матрицу декодера. Инициализируем матрицу случайными значениями из нормального распределения со средним значением 0\n",
    "и стандартным отклонением, рассчитанным на основе размера эмбеддинга размер матрицы = (размер эмбеддинга, количество фонем в словаре)'''\n",
    "decoder = np.random.normal(0, 1/np.sqrt(embed_size), size=(embed_size, len(vocab)))\n",
    "\n",
    "'''Создаем матрицу one-hot encoding. Создаем единичную матрицу, которая используется для представления фонем в виде one-hot вектора\n",
    "размер матрицы = (количество фонем в словаре, количество фонем в словаре)'''\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "\n",
    "print('Вектор эмбейдинга:', embed)\n",
    "print('Матрица декодера:', decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция активации \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "# функция активации для обратного распространения\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def predict(word, start, recurrent, decoder, embed, one_hot, dropout_rate = 0.0):\n",
    "    '''Функция принимает на вход список индексов фонем (слова)'''\n",
    "    layers = list() #Список с именем layers для прямого распространения\n",
    "    layer = {} #словарб для слоёв\n",
    "    layer['hidden'] = start #скрытое состояние первого слоя начальным вектором 'start'\n",
    "    layers.append(layer) #добавляем первый слой в список\n",
    "    loss = 0 #для хранения суммарной ошибки\n",
    "    correct_predictions = 0 #правильные предсказания\n",
    "    total_predictions = 0 #все предсказания\n",
    "\n",
    "    # итерация по слову\n",
    "    for target_i in range(len(word)): #проходим по всем фонемам в слове\n",
    "        layer = {} #словарь для этого слоя\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) #извлекает вероятность, которую сеть предсказала для следующей фонемы\n",
    "\n",
    "        # вычисляем ошибку для текущего предсказания, используя отрицательный логарифм вероятности правильного слова, добавляем ошибку к общей суммарной ошибке 'loss'\n",
    "        loss += -np.log(layer['pred'][word[target_i]]) #чем ниже вероятность предсказанного слова, тем выше значение loss\n",
    "\n",
    "        # вычисляем скрытое состояние с использованием функции активации tanh\n",
    "        hidden = tanh(layers[-1]['hidden'].dot(recurrent) + embed[word[target_i]])\n",
    "\n",
    "        # drop_out\n",
    "        # dropout_rate - это вероятность отбрасывания нейрона\n",
    "        if dropout_rate > 0: #если dropout больше нуля то он применяется\n",
    "            # создаем бинарную маску как у скрытого слоя и имеет элементы равные единице с вероятностью 1 минус dropout_rate и 0 с вероятностью dropout_rate\n",
    "            # некоторые нейроны будут умножены на 0 (выключены), а остальные будут умножены на 1 (включены)\n",
    "            # для обратного распространения градиента, делим маску на (1 - dropout_rate)\n",
    "            # это приводит к тому что суммарное значение после dropout, такое же как и до dropout\n",
    "            dropout_mask = np.random.binomial(1, 1 - dropout_rate, size=hidden.shape) / (1 - dropout_rate)\n",
    "            # применяем его к скрытому слою умножая на маску\n",
    "            hidden_dropout = hidden * dropout_mask\n",
    "        else:\n",
    "            # если dropout = 0, то маска не применяется \n",
    "            hidden_dropout = hidden\n",
    "\n",
    "        # записываем результаты в скрытый слой и скрытый слой с dropout\n",
    "        layer['hidden'] = hidden\n",
    "        layer['hidden_dropout'] = hidden_dropout\n",
    "        layers.append(layer) \n",
    "\n",
    "        predicted_index = np.argmax(layer['pred']) #предсказываем индекс следующей фонемы\n",
    "        if predicted_index == word[target_i]: #если предсказанная фонема совпадает с правильной, то увеличиваем количество верных предсказаний\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1 #увеличиваем общее количество предсказаний\n",
    "\n",
    "    # вычисляем точность модели\n",
    "    # делим количество верных предсказаний на общее количество предсказаний\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    # возвращаем результат (слои потерю и точность)\n",
    "    return layers, loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем папку для сохранения весов молели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь для сохранения весов модели\n",
    "save_path = \"weights\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path) #если папки нет создаем \n",
    "# Переменные для хранения лучших результатов модели\n",
    "best_accuracy = 0.0 #\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1/100, Обучение: Loss: 21.434, Accuracy: 0.018 , Тест: Loss: 21.415, Accuracy: 0.021\n",
      "Веса обновлены\n",
      "Эпоха 2/100, Обучение: Loss: 21.399, Accuracy: 0.025 , Тест: Loss: 21.381, Accuracy: 0.029\n",
      "Веса обновлены\n",
      "Эпоха 3/100, Обучение: Loss: 21.365, Accuracy: 0.029 , Тест: Loss: 21.348, Accuracy: 0.030\n",
      "Веса обновлены\n",
      "Эпоха 4/100, Обучение: Loss: 21.330, Accuracy: 0.032 , Тест: Loss: 21.311, Accuracy: 0.034\n",
      "Веса обновлены\n",
      "Эпоха 5/100, Обучение: Loss: 21.290, Accuracy: 0.036 , Тест: Loss: 21.267, Accuracy: 0.038\n",
      "Веса обновлены\n",
      "Эпоха 6/100, Обучение: Loss: 21.240, Accuracy: 0.041 , Тест: Loss: 21.210, Accuracy: 0.048\n",
      "Веса обновлены\n",
      "Эпоха 7/100, Обучение: Loss: 21.173, Accuracy: 0.051 , Тест: Loss: 21.132, Accuracy: 0.056\n",
      "Веса обновлены\n",
      "Эпоха 8/100, Обучение: Loss: 21.081, Accuracy: 0.063 , Тест: Loss: 21.025, Accuracy: 0.070\n",
      "Веса обновлены\n",
      "Эпоха 9/100, Обучение: Loss: 20.957, Accuracy: 0.077 , Тест: Loss: 20.882, Accuracy: 0.081\n",
      "Веса обновлены\n",
      "Эпоха 10/100, Обучение: Loss: 20.797, Accuracy: 0.086 , Тест: Loss: 20.705, Accuracy: 0.088\n",
      "Веса обновлены\n",
      "Эпоха 11/100, Обучение: Loss: 20.606, Accuracy: 0.093 , Тест: Loss: 20.503, Accuracy: 0.095\n",
      "Веса обновлены\n",
      "Эпоха 12/100, Обучение: Loss: 20.398, Accuracy: 0.104 , Тест: Loss: 20.292, Accuracy: 0.115\n",
      "Веса обновлены\n",
      "Эпоха 13/100, Обучение: Loss: 20.190, Accuracy: 0.130 , Тест: Loss: 20.090, Accuracy: 0.134\n",
      "Веса обновлены\n",
      "Эпоха 14/100, Обучение: Loss: 19.998, Accuracy: 0.136 , Тест: Loss: 19.908, Accuracy: 0.136\n",
      "Веса обновлены\n",
      "Эпоха 15/100, Обучение: Loss: 19.827, Accuracy: 0.137 , Тест: Loss: 19.748, Accuracy: 0.137\n",
      "Веса обновлены\n",
      "Эпоха 16/100, Обучение: Loss: 19.677, Accuracy: 0.137 , Тест: Loss: 19.606, Accuracy: 0.137\n",
      "Эпоха 17/100, Обучение: Loss: 19.541, Accuracy: 0.137 , Тест: Loss: 19.476, Accuracy: 0.136\n",
      "Эпоха 18/100, Обучение: Loss: 19.417, Accuracy: 0.137 , Тест: Loss: 19.357, Accuracy: 0.136\n",
      "Эпоха 19/100, Обучение: Loss: 19.302, Accuracy: 0.136 , Тест: Loss: 19.246, Accuracy: 0.136\n",
      "Эпоха 20/100, Обучение: Loss: 19.196, Accuracy: 0.136 , Тест: Loss: 19.145, Accuracy: 0.136\n",
      "Эпоха 21/100, Обучение: Loss: 19.099, Accuracy: 0.136 , Тест: Loss: 19.051, Accuracy: 0.136\n",
      "Эпоха 22/100, Обучение: Loss: 19.010, Accuracy: 0.136 , Тест: Loss: 18.966, Accuracy: 0.136\n",
      "Эпоха 23/100, Обучение: Loss: 18.929, Accuracy: 0.136 , Тест: Loss: 18.888, Accuracy: 0.136\n",
      "Эпоха 24/100, Обучение: Loss: 18.855, Accuracy: 0.136 , Тест: Loss: 18.817, Accuracy: 0.136\n",
      "Эпоха 25/100, Обучение: Loss: 18.782, Accuracy: 0.136 , Тест: Loss: 18.741, Accuracy: 0.136\n",
      "Эпоха 26/100, Обучение: Loss: 18.698, Accuracy: 0.145 , Тест: Loss: 18.649, Accuracy: 0.145\n",
      "Веса обновлены\n",
      "Эпоха 27/100, Обучение: Loss: 18.601, Accuracy: 0.145 , Тест: Loss: 18.551, Accuracy: 0.145\n",
      "Веса обновлены\n",
      "Эпоха 28/100, Обучение: Loss: 18.509, Accuracy: 0.144 , Тест: Loss: 18.469, Accuracy: 0.144\n",
      "Эпоха 29/100, Обучение: Loss: 18.439, Accuracy: 0.144 , Тест: Loss: 18.410, Accuracy: 0.144\n",
      "Эпоха 30/100, Обучение: Loss: 18.389, Accuracy: 0.160 , Тест: Loss: 18.368, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 31/100, Обучение: Loss: 18.351, Accuracy: 0.160 , Тест: Loss: 18.333, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 32/100, Обучение: Loss: 18.319, Accuracy: 0.160 , Тест: Loss: 18.303, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 33/100, Обучение: Loss: 18.291, Accuracy: 0.160 , Тест: Loss: 18.278, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 34/100, Обучение: Loss: 18.271, Accuracy: 0.160 , Тест: Loss: 18.263, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 35/100, Обучение: Loss: 18.262, Accuracy: 0.160 , Тест: Loss: 18.262, Accuracy: 0.160\n",
      "Веса обновлены\n",
      "Эпоха 36/100, Обучение: Loss: 18.268, Accuracy: 0.160 , Тест: Loss: 18.274, Accuracy: 0.160\n",
      "Эпоха 37/100, Обучение: Loss: 18.287, Accuracy: 0.160 , Тест: Loss: 18.298, Accuracy: 0.160\n",
      "Эпоха 38/100, Обучение: Loss: 18.316, Accuracy: 0.160 , Тест: Loss: 18.333, Accuracy: 0.160\n",
      "Эпоха 39/100, Обучение: Loss: 18.355, Accuracy: 0.160 , Тест: Loss: 18.377, Accuracy: 0.160\n",
      "Эпоха 40/100, Обучение: Loss: 18.405, Accuracy: 0.160 , Тест: Loss: 18.432, Accuracy: 0.160\n",
      "Эпоха 41/100, Обучение: Loss: 18.463, Accuracy: 0.160 , Тест: Loss: 18.493, Accuracy: 0.160\n",
      "Эпоха 42/100, Обучение: Loss: 18.528, Accuracy: 0.160 , Тест: Loss: 18.562, Accuracy: 0.160\n",
      "Эпоха 43/100, Обучение: Loss: 18.599, Accuracy: 0.160 , Тест: Loss: 18.634, Accuracy: 0.160\n",
      "Эпоха 44/100, Обучение: Loss: 18.671, Accuracy: 0.160 , Тест: Loss: 18.705, Accuracy: 0.160\n",
      "Эпоха 45/100, Обучение: Loss: 18.740, Accuracy: 0.160 , Тест: Loss: 18.771, Accuracy: 0.160\n",
      "Эпоха 46/100, Обучение: Loss: 18.804, Accuracy: 0.160 , Тест: Loss: 18.833, Accuracy: 0.160\n",
      "Эпоха 47/100, Обучение: Loss: 18.863, Accuracy: 0.160 , Тест: Loss: 18.889, Accuracy: 0.160\n",
      "Эпоха 48/100, Обучение: Loss: 18.916, Accuracy: 0.160 , Тест: Loss: 18.939, Accuracy: 0.160\n",
      "Эпоха 49/100, Обучение: Loss: 18.962, Accuracy: 0.160 , Тест: Loss: 18.982, Accuracy: 0.160\n",
      "Эпоха 50/100, Обучение: Loss: 19.003, Accuracy: 0.160 , Тест: Loss: 19.020, Accuracy: 0.160\n",
      "Эпоха 51/100, Обучение: Loss: 19.039, Accuracy: 0.160 , Тест: Loss: 19.053, Accuracy: 0.160\n",
      "Эпоха 52/100, Обучение: Loss: 19.070, Accuracy: 0.160 , Тест: Loss: 19.083, Accuracy: 0.160\n",
      "Эпоха 53/100, Обучение: Loss: 19.098, Accuracy: 0.160 , Тест: Loss: 19.109, Accuracy: 0.160\n",
      "Эпоха 54/100, Обучение: Loss: 19.123, Accuracy: 0.160 , Тест: Loss: 19.134, Accuracy: 0.160\n",
      "Эпоха 55/100, Обучение: Loss: 19.148, Accuracy: 0.160 , Тест: Loss: 19.157, Accuracy: 0.160\n",
      "Эпоха 56/100, Обучение: Loss: 19.170, Accuracy: 0.160 , Тест: Loss: 19.178, Accuracy: 0.160\n",
      "Эпоха 57/100, Обучение: Loss: 19.191, Accuracy: 0.160 , Тест: Loss: 19.198, Accuracy: 0.160\n",
      "Эпоха 58/100, Обучение: Loss: 19.210, Accuracy: 0.160 , Тест: Loss: 19.216, Accuracy: 0.160\n",
      "Эпоха 59/100, Обучение: Loss: 19.227, Accuracy: 0.160 , Тест: Loss: 19.233, Accuracy: 0.160\n",
      "Эпоха 60/100, Обучение: Loss: 19.243, Accuracy: 0.160 , Тест: Loss: 19.248, Accuracy: 0.160\n",
      "Эпоха 61/100, Обучение: Loss: 19.259, Accuracy: 0.160 , Тест: Loss: 19.263, Accuracy: 0.160\n",
      "Эпоха 62/100, Обучение: Loss: 19.273, Accuracy: 0.160 , Тест: Loss: 19.277, Accuracy: 0.160\n",
      "Эпоха 63/100, Обучение: Loss: 19.287, Accuracy: 0.160 , Тест: Loss: 19.290, Accuracy: 0.160\n",
      "Эпоха 64/100, Обучение: Loss: 19.300, Accuracy: 0.160 , Тест: Loss: 19.303, Accuracy: 0.160\n",
      "Эпоха 65/100, Обучение: Loss: 19.314, Accuracy: 0.160 , Тест: Loss: 19.319, Accuracy: 0.160\n",
      "Эпоха 66/100, Обучение: Loss: 19.330, Accuracy: 0.160 , Тест: Loss: 19.334, Accuracy: 0.160\n",
      "Эпоха 67/100, Обучение: Loss: 19.347, Accuracy: 0.160 , Тест: Loss: 19.352, Accuracy: 0.160\n",
      "Эпоха 68/100, Обучение: Loss: 19.365, Accuracy: 0.160 , Тест: Loss: 19.369, Accuracy: 0.160\n",
      "Эпоха 69/100, Обучение: Loss: 19.383, Accuracy: 0.160 , Тест: Loss: 19.387, Accuracy: 0.160\n",
      "Эпоха 70/100, Обучение: Loss: 19.399, Accuracy: 0.160 , Тест: Loss: 19.402, Accuracy: 0.160\n",
      "Эпоха 71/100, Обучение: Loss: 19.415, Accuracy: 0.160 , Тест: Loss: 19.417, Accuracy: 0.160\n",
      "Эпоха 72/100, Обучение: Loss: 19.430, Accuracy: 0.160 , Тест: Loss: 19.432, Accuracy: 0.160\n",
      "Эпоха 73/100, Обучение: Loss: 19.444, Accuracy: 0.160 , Тест: Loss: 19.445, Accuracy: 0.160\n",
      "Эпоха 74/100, Обучение: Loss: 19.458, Accuracy: 0.160 , Тест: Loss: 19.459, Accuracy: 0.160\n",
      "Эпоха 75/100, Обучение: Loss: 19.471, Accuracy: 0.160 , Тест: Loss: 19.472, Accuracy: 0.160\n",
      "Эпоха 76/100, Обучение: Loss: 19.484, Accuracy: 0.160 , Тест: Loss: 19.484, Accuracy: 0.160\n",
      "Эпоха 77/100, Обучение: Loss: 19.496, Accuracy: 0.160 , Тест: Loss: 19.496, Accuracy: 0.160\n",
      "Эпоха 78/100, Обучение: Loss: 19.508, Accuracy: 0.160 , Тест: Loss: 19.506, Accuracy: 0.160\n",
      "Эпоха 79/100, Обучение: Loss: 19.518, Accuracy: 0.160 , Тест: Loss: 19.516, Accuracy: 0.160\n",
      "Эпоха 80/100, Обучение: Loss: 19.528, Accuracy: 0.160 , Тест: Loss: 19.525, Accuracy: 0.160\n",
      "Эпоха 81/100, Обучение: Loss: 19.536, Accuracy: 0.160 , Тест: Loss: 19.534, Accuracy: 0.160\n",
      "Эпоха 82/100, Обучение: Loss: 19.545, Accuracy: 0.160 , Тест: Loss: 19.541, Accuracy: 0.160\n",
      "Эпоха 83/100, Обучение: Loss: 19.552, Accuracy: 0.160 , Тест: Loss: 19.548, Accuracy: 0.160\n",
      "Эпоха 84/100, Обучение: Loss: 19.559, Accuracy: 0.160 , Тест: Loss: 19.555, Accuracy: 0.160\n",
      "Эпоха 85/100, Обучение: Loss: 19.566, Accuracy: 0.160 , Тест: Loss: 19.561, Accuracy: 0.160\n",
      "Эпоха 86/100, Обучение: Loss: 19.572, Accuracy: 0.160 , Тест: Loss: 19.568, Accuracy: 0.160\n",
      "Эпоха 87/100, Обучение: Loss: 19.578, Accuracy: 0.160 , Тест: Loss: 19.574, Accuracy: 0.160\n",
      "Эпоха 88/100, Обучение: Loss: 19.584, Accuracy: 0.160 , Тест: Loss: 19.579, Accuracy: 0.160\n",
      "Эпоха 89/100, Обучение: Loss: 19.590, Accuracy: 0.160 , Тест: Loss: 19.585, Accuracy: 0.160\n",
      "Эпоха 90/100, Обучение: Loss: 19.596, Accuracy: 0.160 , Тест: Loss: 19.591, Accuracy: 0.160\n",
      "Эпоха 91/100, Обучение: Loss: 19.601, Accuracy: 0.160 , Тест: Loss: 19.597, Accuracy: 0.160\n",
      "Эпоха 92/100, Обучение: Loss: 19.607, Accuracy: 0.160 , Тест: Loss: 19.602, Accuracy: 0.160\n",
      "Эпоха 93/100, Обучение: Loss: 19.613, Accuracy: 0.160 , Тест: Loss: 19.608, Accuracy: 0.160\n",
      "Эпоха 94/100, Обучение: Loss: 19.618, Accuracy: 0.160 , Тест: Loss: 19.613, Accuracy: 0.160\n",
      "Эпоха 95/100, Обучение: Loss: 19.624, Accuracy: 0.160 , Тест: Loss: 19.619, Accuracy: 0.160\n",
      "Эпоха 96/100, Обучение: Loss: 19.629, Accuracy: 0.160 , Тест: Loss: 19.624, Accuracy: 0.160\n",
      "Эпоха 97/100, Обучение: Loss: 19.635, Accuracy: 0.160 , Тест: Loss: 19.630, Accuracy: 0.160\n",
      "Эпоха 98/100, Обучение: Loss: 19.640, Accuracy: 0.160 , Тест: Loss: 19.635, Accuracy: 0.160\n",
      "Эпоха 99/100, Обучение: Loss: 19.646, Accuracy: 0.160 , Тест: Loss: 19.641, Accuracy: 0.160\n",
      "Эпоха 100/100, Обучение: Loss: 19.651, Accuracy: 0.160 , Тест: Loss: 19.646, Accuracy: 0.160\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # создаем батчи из обучающих данных\n",
    "    train_batches = create_batches(train_tokens, BATCH_SIZE, phon2index)\n",
    "   \n",
    "    # задаем переменню общей ошибки, точности, слов \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_words = 0 \n",
    "\n",
    "    for batch in train_batches: #цикл для бача на обучающей выборке (задаем все по нулям)\n",
    "        batch_loss = 0 \n",
    "        batch_accuracy = 0 \n",
    "        batch_words = 0\n",
    "        \n",
    "        for word in batch: #цикл для каждого слова в батче\n",
    "            '''Получаем результаты предсказания модели:\n",
    "            layers: список слоев, содержащих предсказанные значения и другие данные\n",
    "            loss: значение ошибки для данного слова\n",
    "            accuracy: значение точности для данного слова'''\n",
    "            layers, loss, accuracy = predict(word, start, recurrent, decoder, embed, one_hot, dropout_rate=0.2)            \n",
    "            batch_loss += loss\n",
    "            batch_accuracy += accuracy\n",
    "            batch_words += 1\n",
    "                \n",
    "            # обратное распространение для обновления весов (вычисляем градиенты ошибки и используем их для корректировки весов модели)\n",
    "            for layer_idx in reversed(range(len(layers))):\n",
    "                # итерируем по слоям в обратном порядке\n",
    "                layer = layers[layer_idx] \n",
    "                # получаем текущий слой\n",
    "                target = word[layer_idx-1]\n",
    "\n",
    "                # проверяем, что слой не первый (входной)\n",
    "                if(layer_idx > 0): \n",
    "                    layer['output_delta'] = layer['pred'] - one_hot[target] #вычисляем разницу между предсказанным и ожидаемым вектором\n",
    "                    new_hidden_delta = layer['output_delta'].dot(decoder.transpose()) #вычисляем ошибку скрытого состояния текущего слоя (транспонируя матрицу весов decoder и перемножая с дельтой выходного слоя)\n",
    "                    \n",
    "                    # проверяем, что слой не последний (выходной)\n",
    "                    if(layer_idx == len(layers)-1):\n",
    "                        layer['hidden_delta'] = new_hidden_delta\n",
    "                    else:\n",
    "                        # ошибка скрытого состояния вычисляется на основе ошибки текущего слоя и ошибки скрытого состояния следующего слоя\n",
    "                        layer['hidden_delta'] = (new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())) * tanh_deriv(layers[layer_idx]['hidden'])\n",
    "                else: #если слой первый\n",
    "                    # вычисляем ошибку скрытого состояния первого слоя на основе ошибки скрытого состояния второго слоя\n",
    "                    layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose()) * tanh_deriv(layers[layer_idx]['hidden'])\n",
    "\n",
    "            # Ограничение градиента (для предотвращения градиентного взрыва)\n",
    "            for layer in layers: #цикл по слоям\n",
    "                if 'hidden_delta' in layer: #проверяем, есть ли ошибка скрытого слоя в текущем слое\n",
    "                    grad_norm = np.linalg.norm(layer['hidden_delta']) # вычисляем норму градиента\n",
    "                    if grad_norm > 10: #устанавливаем максимальную норму\n",
    "                        layer['hidden_delta'] = layer['hidden_delta'] * (10 / grad_norm) # нормализуем градиент если он большой\n",
    "\n",
    "            '''теперь корректируем веса'''\n",
    "            # обновляем начальное скрытое состояние на основе ошибки скрытого состояния первого слоя, деля на длину предложения для нормализации\n",
    "            # вычитаем из начального скрытого состояния (start) произведение ошибки скрытого состояния первого слоя, скорости обучения и деленное на длину предложения.\n",
    "            start -= layers[0]['hidden_delta'] * learning_rate / (len(word))\n",
    "            # итерация по всем слоям со второго \n",
    "            for layer_idx,layer in enumerate(layers[1:]):\n",
    "                # корректируем матрицу декодера с учетом скрытого состояния текущего слоя и ошибки выходного слоя, деля на длину слова для нормализации\n",
    "                decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * learning_rate / (len(word))\n",
    "                # извлекаем индекс текущего слова из списка индексов предложения\n",
    "                embed_idx = word[layer_idx]\n",
    "                # обновляем вектор эмбеддинга текущего слова (embed[embed_idx]), вычитая из текущего значения вектора произведение ошибки скрытого состояния, скорости обучения и деленное на длину слова\n",
    "                # корректирует вектор эмбеддинга для уменьшения ошибки\n",
    "                embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * learning_rate / (len(word)) #корректируем вектор эмбеддинга текущего слова с помощью ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "                # обновляем матрицу рекуррентной связи, вычитая из текущего значения матрицы внешнее произведение скрытого состояния и ошибки скрытого состояния, умноженное на скорость обучения и деленное на длину предложения\n",
    "                # корректирует матрицу рекуррентной связи для уменьшения ошибки при обновлении скрытого состояния\n",
    "                recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * learning_rate / (len(word)) #обновляем матрицу рекуррентной связи с помощью скрытого состояния текущего слоя и ошибки скрытого состояния текущего слоя, деля на длину предложения для нормализации\n",
    "            \n",
    "        if batch_words > 0: #проверка на пустые батчи\n",
    "            total_loss += batch_loss #суммируем ошибки для всего батча\n",
    "            total_accuracy += batch_accuracy #суммируем точность для всего батча\n",
    "            total_words += batch_words #суммируем количество слов в батче\n",
    "    if total_words > 0: #проверяем было ли обработано хоть одно слово в текущей эпохе\n",
    "         avg_loss = total_loss/total_words #среднее значение ошибки\n",
    "         avg_accuracy = total_accuracy/total_words #среднее значение точности\n",
    "    else: #если в эпохе не было обработано слов, то средние значения равны нулю\n",
    "         avg_loss = 0\n",
    "         avg_accuracy = 0\n",
    "\n",
    "    # оценка модели на тестовой выборке\n",
    "    test_accuracy = 0\n",
    "    test_words = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    # создаем батчи для тестовой выборки\n",
    "    test_batches = create_batches(train_tokens, BATCH_SIZE, phon2index)\n",
    "    # цикл по тестовым батчам \n",
    "    for test_batch in test_batches:\n",
    "        batch_test_accuracy = 0 #переменная для хранения точности текущего тестового батча\n",
    "        batch_test_loss = 0 #переменная для хранения потери текущего тестового батча\n",
    "        batch_test_words = 0 #переменняа для хранения количсетва слов в тестовом батче\n",
    "\n",
    "        # цикл по всем словам в тестовом батче\n",
    "        for word in test_batch:\n",
    "            layers, loss, accuracy = predict(word, start, recurrent, decoder, embed, one_hot, dropout_rate=0.2)            \n",
    "            batch_test_accuracy += accuracy #добавляем точность \n",
    "            batch_test_loss += loss #добавляем потерю\n",
    "            batch_test_words += 1 #бобавляем счет слов\n",
    "\n",
    "        # проверка на наличие хотя бы одного слова в батче и так же суммируем\n",
    "        if batch_test_words > 0:\n",
    "            test_accuracy += batch_test_accuracy \n",
    "            test_loss += batch_test_loss\n",
    "            test_words += batch_test_words\n",
    "\n",
    "    # проверяем, было ли обработано хотя бы одно слово в тестовой выборке\n",
    "    if test_words > 0:\n",
    "        avg_test_accuracy = test_accuracy/test_words #вычисляем среднюю точность, деля тестовую точность на количество слов\n",
    "        avg_test_loss = test_loss/test_words #вычисляем среднюю ошибку, деля тестовую ошибку на количество слов\n",
    "    # если не было обработано ни одного слова средние значения 0\n",
    "    else:\n",
    "        avg_test_accuracy = 0\n",
    "        avg_test_loss = 0\n",
    "          \n",
    "    # вывод процесса обучения\n",
    "    print(f\"Эпоха {epoch + 1}/{EPOCHS}, \"\n",
    "          f\"Обучение: Loss: {avg_loss:.3f}, Accuracy: {avg_accuracy:.3f} , \"\n",
    "          f\"Тест: Loss: {avg_test_loss:.3f}, Accuracy: {avg_test_accuracy:.3f}\")\n",
    "    # прерываем обучение если среднняя ошибка nan \n",
    "    if np.isnan(avg_loss): \n",
    "        print('Ошибка: nan')\n",
    "        break  \n",
    "    \n",
    "    # проверка на улучшение\n",
    "    # ниже мы проверяем улучшилась ли модель от эпохи к эпохе \n",
    "    # устанавливая условие - либо текущая точность на тестовой выборке (avg_test_accuracy) больше лучшей предыдущей (best_accuracy), \n",
    "    # либо текущая точность равна лучшей, но текущая ошибка на тестовой выборке (avg_test_loss) меньше лучшей предыдущей (best_loss)\n",
    "    if avg_test_accuracy > best_accuracy or (avg_test_accuracy == best_accuracy and avg_test_loss < best_loss):\n",
    "            # обновляем лучшую точность на ту, которую получили по условию\n",
    "            best_accuracy = avg_test_accuracy\n",
    "            # так же с потерей\n",
    "            best_loss = avg_test_loss\n",
    "            # сохранение весов в нашу папку для весов\n",
    "            np.save(os.path.join(save_path, \"embed.npy\"), embed)\n",
    "            np.save(os.path.join(save_path, \"recurrent.npy\"), recurrent)\n",
    "            np.save(os.path.join(save_path, \"start.npy\"), start)\n",
    "            np.save(os.path.join(save_path, \"decoder.npy\"), decoder)\n",
    "            print('Веса обновлены')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попросим сеть предсказать следующую фонему в слове с индексом n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка весов модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(save_path):\n",
    "    \"\"\"Функция для загрузки весов модели\"\"\"\n",
    "    embed = np.load(os.path.join(save_path, \"embed.npy\"))\n",
    "    recurrent = np.load(os.path.join(save_path, \"recurrent.npy\"))\n",
    "    start = np.load(os.path.join(save_path, \"start.npy\"))\n",
    "    decoder = np.load(os.path.join(save_path, \"decoder.npy\"))\n",
    "    return embed, recurrent, start, decoder\n",
    "# определяем веса из загруженных данных\n",
    "embed, recurrent, start, decoder = load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'r', 'a', \"l'\", 'i', 'zh', 'e', 'l', 'a']\n",
      "Предыдущая фонема: p Истинная: r Предсказанная: a\n",
      "Предыдущая фонема: r Истинная: a Предсказанная: a\n",
      "Предыдущая фонема: a Истинная: l' Предсказанная: a\n",
      "Предыдущая фонема: l' Истинная: i Предсказанная: a\n",
      "Предыдущая фонема: i Истинная: zh Предсказанная: a\n",
      "Предыдущая фонема: zh Истинная: e Предсказанная: a\n",
      "Предыдущая фонема: e Истинная: l Предсказанная: a\n",
      "Предыдущая фонема: l Истинная: a Предсказанная: a\n"
     ]
    }
   ],
   "source": [
    "word_index = 15\n",
    "# получаем предсказания модели для слова с индексом word_index, преобразуя его фонемы в числовые индексы\n",
    "embed, recurrent, start, decoder = load_model(save_path)\n",
    "l, _, _ = predict(phons2indices(tokens[word_index], phon2index), start, recurrent, decoder, embed, one_hot)\n",
    "print(tokens[word_index])\n",
    "# цикл по всем слоям в списке предсказаний (l), кроме входного и выходного слоя \n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    # получаем входную фонему для текущего слоя из оригинального слова\n",
    "    input_phon = tokens[word_index][i]\n",
    "    # получаем настоящую фонему для текущего слоя из оригинального слова\n",
    "    true_phon = tokens[word_index][i + 1]\n",
    "    # получаем предсказаную фонему для текущего слоя из оригинального слова (argmax() используется для нахождения индекса максимальной вероятности в предсказании)\n",
    "    pred_phon = vocab[each_layer['pred'].argmax()]\n",
    "    print(f\"Предыдущая фонема: {input_phon} Истинная: {true_phon} Предсказанная: {pred_phon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'a', 't', \"v'\", 'e', 'r', \"d'\", 'i', 'l', 'a', \"s'\"]\n",
      "Предыдущая фонема: p Истинная: a Предсказанная: a\n",
      "Предыдущая фонема: a Истинная: t Предсказанная: a\n",
      "Предыдущая фонема: t Истинная: v' Предсказанная: a\n",
      "Предыдущая фонема: v' Истинная: e Предсказанная: a\n",
      "Предыдущая фонема: e Истинная: r Предсказанная: a\n",
      "Предыдущая фонема: r Истинная: d' Предсказанная: a\n",
      "Предыдущая фонема: d' Истинная: i Предсказанная: a\n",
      "Предыдущая фонема: i Истинная: l Предсказанная: a\n",
      "Предыдущая фонема: l Истинная: a Предсказанная: a\n",
      "Предыдущая фонема: a Истинная: s' Предсказанная: a\n"
     ]
    }
   ],
   "source": [
    "word_index = 25\n",
    "# получаем предсказания модели для слова с индексом word_index, преобразуя его фонемы в числовые индексы\n",
    "embed, recurrent, start, decoder = load_model(save_path)\n",
    "l, _, _ = predict(phons2indices(tokens[word_index], phon2index), start, recurrent, decoder, embed, one_hot)\n",
    "print(tokens[word_index])\n",
    "# цикл по всем слоям в списке предсказаний (l), кроме входного и выходного слоя \n",
    "for i, each_layer in enumerate(l[1:-1]):\n",
    "    # получаем входную фонему для текущего слоя из оригинального слова\n",
    "    input_phon = tokens[word_index][i]\n",
    "    # получаем настоящую фонему для текущего слоя из оригинального слова\n",
    "    true_phon = tokens[word_index][i + 1]\n",
    "    # получаем предсказаную фонему для текущего слоя из оригинального слова (argmax() используется для нахождения индекса максимальной вероятности в предсказании)\n",
    "    pred_phon = vocab[each_layer['pred'].argmax()]\n",
    "    print(f\"Предыдущая фонема: {input_phon} Истинная: {true_phon} Предсказанная: {pred_phon}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
